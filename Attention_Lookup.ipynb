{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\contextcsm}{\\mathcal{c}}\n",
    "\\newcommand{\\querycsm}{\\mathcal{q}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Implementing attention: High level view\n",
    "\n",
    "To state the problem of Attention more abstractly as follows\n",
    "\n",
    "Given\n",
    "- Source sequence $\\bar{\\contextcsm}_{([1:\\bar{T}])}$\n",
    "    - the sequence being \"attended to\"\n",
    "    - a sequence of source \"contexts\"\n",
    "- and a Target context $\\contextcsm_\\tp$ \n",
    "    - called the \"query\"\n",
    "\n",
    "Output\n",
    "- the Source context $\\bar{\\contextcsm}_{(\\bar{\\tt})}$\n",
    "- that most closely matches the desired Target context $\\contextcsm_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, let's consider Cross Attention in an Encoder-Decoder architecture\n",
    "- $\\bar{\\contextcsm}_{([1:\\bar{T}])}$ may be the sequence of latent states of an Encoder\n",
    "- \"query\" $\\contextcsm_\\tp = \\h_\\tp$ is the state of the Decoder when generating output $\\hat\\y_\\tp$ at position $\\tt$\n",
    "- we want to output $\\bar{\\contextcsm}_{(\\bar \\tt)}$: one latent state of the Encoder\n",
    "    - relevant for output position $\\tt$\n",
    "    - as described by  $\\contextcsm_\\tp = \\h_\\tp$ \n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation with attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The mechanism we use to match Target and Source contexts is called *Context Sensitive Memory*.\n",
    "\n",
    "Summary\n",
    "- Context Sensitive Memory is similar to a Python `dict`\n",
    "    - consists of a collection of Key/Value pairs\n",
    "- One may perform a \"lookup\"\n",
    "    - By presenting a \"query\"\n",
    "    - Which matches the query against each key\n",
    "- The result is a \"soft\" lookup\n",
    "    - always returns a value, even if there is no exact match between the query and any key\n",
    "    - the results is a weighted sum of the values in the key/value pairs\n",
    "    - with weights based on the similarity of the query and the key\n",
    "    \n",
    "Let's see how [Context Sensitive Memory](Context_Sensitive_Memory.ipynb) works.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cross-Attention lookup: detailed view\n",
    "\n",
    "In general the keys, values and queries could be generated by arbitrary parts of a larger\n",
    "Neural Network that uses Attention.\n",
    "\n",
    "In the case of an Encoder-Decoder architecture\n",
    "the Attention is between\n",
    "- queries created by the Decoder\n",
    "- keys and values created by the Encoder\n",
    "    - keys and values are identical\n",
    "\n",
    "We use a Context Sensitive Memory to implement the Attention lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The CSM has $\\bar T$ key/value pairs\n",
    "- one for each output position of the Encoder: $\\bar \\h_{(1 \\ldots \\bar T)}$\n",
    "- the key and value for position $\\bar \\tt$ of the CSM is state $\\bar \\h_\\tp$\n",
    "$$k_{\\bar \\tt} = v_{\\bar \\tt} = \\bar \\h_{(\\bar \\tt)}$$\n",
    "\n",
    "The Decoder creates one query for each of the $T$ positions of the Decoder output\n",
    "- the query for position $\\tt$ is Decoder state $\\h_\\tp$\n",
    "$$q_\\tt = \\h_\\tp$$\n",
    "\n",
    "Thus, each position of the Decoder\n",
    "- attends to all positions of the Encoder\n",
    "- using Decoder state $\\h_\\tp$ as the query for output position $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an illustration of the Attention inputs of the Encoder Decoder.\n",
    "- left row bottom: sequence of latent states of the Encoder\n",
    "    - used as keys/values:\n",
    "        - sequence length: $\\bar T$ for Cross-Attention; $T$ for Self-Attention\n",
    "- right row botton: sequence of latent states of the Decoder\n",
    "    - used as queries\n",
    "    - sequence length: $T$\n",
    "- top row: attention output\n",
    "    - weighted sum of values\n",
    "- Attention Weight matrix entry row $r_e$, column $c_d$\n",
    "    - the weight of query at Decoder position $c_d$ on Encoder position $r_e$  \n",
    "- Top row\n",
    "    - position $\\tt$: sum over column $\\tt$'s (weights * values)\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture of the complete RNN Encoder Decoder designed to translate Spanish to English\n",
    "\n",
    "Both the Encoder and Decoder are RNN's.\n",
    "\n",
    "- Encoder: left side (bottom to top)\n",
    "    - bottom row: sequence of token ids of Spanish language input\n",
    "    - middle row: an unrolled, bidirectional RNN computation\n",
    "        - computing an encoding (latent representation) for each of the $\\bar T$ Spanish tokens\n",
    "    - top row: sequence of latent representations of Spanish tokens\n",
    "        - used as keys/values for Attention\n",
    "- Decoder: similar to Encoder\n",
    "    - top row: latent representation of generated English token ids\n",
    "        - used as queries for Attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>RNN Encoder-Decoder for Spanish to English translation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN%2Battention-words-spa.png\" width=30%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention Lookup: general case\n",
    "\n",
    "We assume that \n",
    "- the Source context (the sequence being attended to) is length $\\bar T$\n",
    "    - e.g., Encoder states $\\bar\\h_\\tp$ in an Encoder/Decoder\n",
    "- the Target context is length $T$\n",
    "    - e.g., Decoder states $\\h_\\tp$ in an Encoder/Decoder\n",
    "\n",
    "Each element in the vectors ($\\h, \\bar\\h$) are length $d$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "| \\bar \\h_{(\\bar \\tt)} |  & = & d &  1 \\le \\bar \\tt \\le \\bar T \\\\\n",
    "| \\h_\\tp | & = & d & 1 \\le \\tt \\le T \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This describes Cross-Attention as would be implemented from the Decoder to the Encoder\n",
    "in an Encoder-Decoder architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the special case of Self-Attention: \n",
    "- $\\bar T = T$\n",
    "- $\\bar\\h_\\tp = \\h_\\tp$\n",
    "\n",
    "This is the case, for example, where a Decoder attends to itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Queries\n",
    "\n",
    "Each of the $T$ Target positions is a query\n",
    "\n",
    "$$\n",
    "q_\\tp = h_\\tp\n",
    "$$\n",
    "\n",
    "So the matrix $Q$ of all queries is shape $(T \\times d)$\n",
    "\n",
    "## Keys/Values\n",
    "\n",
    "Each of the $\\bar T$ Source positions is both a target and a query\n",
    "$$\n",
    "k_\\tt = \\v_\\tt = \\bar\\h_\\tp\n",
    "$$\n",
    "\n",
    "The matrix of all keys $K$, and the matrix of all values $V$ are shape $(\\bar T \\times d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projecting queries, keys and values\n",
    "\n",
    "Rather than using the raw states of the Source and Target\n",
    "as queries (resp., keys/values)\n",
    "- we can map them through projection/embedding *matrices* $\\W_Q, \\W_K, \\W_V$\n",
    "    - each mapping matrix shape is $(d \\times d)$\n",
    "    - thus, the mapping preserves the shapes of $Q, K, V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Projection matrices $W_K, W_V, W_Q$ are *learned* through training.\n",
    "\n",
    "This mapping potentially increases the power of a Transformer that uses Attention\n",
    "- if no better representation exists: we presumably learned identity matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mapping through these matrices:\n",
    "\n",
    "out  &nbsp;  &nbsp;  &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:-:|:-:|:-:\n",
    "$Q$ | = | $Q$| * |$\\W_Q$ |\n",
    "$(T \\times d)$ | | $(T \\times d)$ | | $(d \\times d)$\n",
    "&nbsp;\n",
    "$K$ | = | $K$| * |$\\W_K$ |\n",
    "$V$ | = | $V$| * |$\\W_V$ |\n",
    "$(\\bar T \\times d)$ | | $(\\bar T \\times d)$ | | $(d \\times d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performing the lookup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Next: comparing the query $q$ at each Target position, to each of the keys at the $\\bar T$ Source positions\n",
    "- producing scores  $\\alpha(q, k)$  that are implemented as dot product (matrix multiplication)\n",
    "\n",
    "out  &nbsp; &nbsp; &nbsp;    &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:--:|:-:|:-:\n",
    "$\\alpha(q, k)$ | = | $Q$ | * |$K^T$ |\n",
    "$(T \\times \\bar T)$ | | $(T \\times d)$ | | $(d \\times \\bar T)$\n",
    "\n",
    "- we ignore the softmax normalization of the weights\n",
    "- we will treat the scores as weights for simplicity of presentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally: take the weighted sum of the values\n",
    "    \n",
    "out  &nbsp; &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp; | left  &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:--:|:-:|:-:\n",
    " | = | $\\alpha(q, k)$ | * |$V$ |\n",
    " |  = | $Q * K^T$ | * | $V$ |\n",
    "$(T \\times d)$ | | $(T \\times \\bar T)$ | | $(\\bar T \\times d)$  \n",
    "\n",
    "producing\n",
    "- a single attention value of length $d$\n",
    "- for each of the $T$ positionsmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Using matrix operations, we are performing *all* $T$ queries simultaneously.\n",
    "\n",
    "The end result is a vector of length $d$\n",
    "- the value being attended to at each of the $T$ Target positions\n",
    "- this value is a weighted sum of the $\\bar T$ Source states\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax} \\left(\n",
    "\\frac{ Q * K^ T }{ \\sqrt{d} } \\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-head attention\n",
    "\n",
    "With a small change, we can have each Target position attend to $n_\\text{head} \\ge 1$ Source positions.\n",
    "- perhaps each of the $n_\\text{head}$ source positions represents a different aspect of the Source sequence\n",
    "- all of which are relevant to the Target output at a position\n",
    "\n",
    "This is called *Multi-head Attention*\n",
    "- $n_\\text{head}$ attention \"heads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is to take each query (of length $d$) and break it into $n_\\text{head}$ pieces of size\n",
    "$$d_\\text{attn} = \\frac{d}{n_\\text{head}}$$\n",
    "\n",
    "Since the length of query and key must match, we do the same for each key.\n",
    "\n",
    "We then perform regular attention lookup $n_\\text{head}$ times (in parallel) using the shorter queries and keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Size of the value\n",
    "\n",
    "Note that we have not mentioned changing the size of the values that are associated with the keys.\n",
    "\n",
    "After the $n_\\text{head}$ lookups, we have $n_\\text{head}$ vectors of length $d$.\n",
    "\n",
    "Yet all of our model layers (including Attention) must produced output vectors of length $d$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most common way of doing this is to break up the values into $n_\\text{head}$ pieces of size $d_\\text{attn}$\n",
    "- same as for key and query\n",
    "\n",
    "We can then concatenate the $n_\\text{head}$ lookup results of size $d_\\text{attn}$ into a single vector of length $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully a picture will help.\n",
    "\n",
    "Note that each head is working on vectors of length $d_\\text{attn} = \\frac{d}{n_\\text{head}}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A less common way of maintaining output vectors of length $d$\n",
    "- maintain the value vectors at original length $d$\n",
    "- *pool* (e.g., add) the $n_\\text{head}$ vectors into a single vector of length $d$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we create the shorter length $d_\\text{attn}$ vectors (pieces of queries, keys, values) ?\n",
    "- by changing the projection matrices $\\W_Q, \\W_K, \\W_V$ to shape $(d \\times d_\\text{attn})$\n",
    "    - one for each head\n",
    "    - $\\W^{(j)}_Q, \\W^{(j)}_K, \\W^{(j)}_V$ are the projection matrices for head $j$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projecting the lookup result\n",
    "\n",
    "In the [original Attention paper, Figure 2](https://arxiv.org/pdf/1706.03762.pdf#page=4)\n",
    "- the attention lookup output\n",
    "- is projected through matrix $\\W_O$ of shape $(d \\times d)$\n",
    "\n",
    "The argument is similar to why we project queries, keys, and values via $\\W_Q, \\W_K, \\W_V$\n",
    "- the *learned* projection potentially increases the power\n",
    "- if not, $\\W_O$ could be learned to be the Identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This projection of output also enables greater flexibility in breaking up the value part of the key/value pairs\n",
    "- We can choose any length\n",
    "- Let the Output projection matrix reduce the size of the concatenated head outputs\n",
    "- to size $d$ as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-head summary\n",
    "\n",
    "The paper summarizes Multi-Head Attention as\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K,V) =  \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{n_\\text{head}}) \\; \\W_O\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\text{head}_j= \\text{Attention}( Q * \\W_Q^{(j)}, K * \\W_K^{(j)}, V * \\W_V^{(j)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Count the weights !\n",
    "\n",
    "The weights/parameters are in the matrices $\\W_Q, \\W_K, \\W_V$ and $\\W_O$\n",
    "- all of size $\\OrderOf{d^2}$, total:\n",
    "$$\n",
    "4 * \\OrderOf{d^2}\n",
    "$$\n",
    "- multiplied by the number of stacked Transformer blocks $n_\\text{layer}$, total:\n",
    "$$\n",
    "4 * n_\\text{layer} * \\OrderOf{d^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For GPT-3\n",
    "- $n_\\text{layer} = 96$\n",
    "- $d_\\text{model} = 12* 1024$\n",
    "\n",
    "Total attention weights\n",
    "$$\n",
    "96 * (12*1024)^2 = 58 \\text{ billion}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced material\n",
    "\n",
    "The remaining sections include code references to models constructed using the Functional API of Keras.\n",
    "\n",
    "Even if you don't understand the code in detail, the intuition it conveys may be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code: RNN Encoder-Decoder\n",
    "\n",
    "The code for the Spanish to English Encoder Decoder can be found in a [TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n",
    "- requires knowledge of Functional models in Keras\n",
    "- Multi-head Attention implemented by a Keras layer\n",
    "    - code not visible directly\n",
    "    - but is a link to source on Githb\n",
    "        - a bit complex since it is production code\n",
    "- Colab notebook you can play with\n",
    "    - substitute your own Spanish sentences as input\n",
    "    - make Attention plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A good web post on implementing MultiHead Attention can be found [here](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)\n",
    "- rather than using $(d_\\text{model} \\times d_\\text{attn})$ embedding matrices to project vectors from $d_\\text{model}$ to $d_\\text{attn}$\n",
    "- it uses `Dense` layers with $d_\\text{attn}$ units to achieve the same\n",
    "- multi-head attention is achieved by *reshaping* the input\n",
    "    - from 3D shape $( \\text{batch_size} \\times T \\times d_\\text{model} )$\n",
    "    - to 4D shape $( \\text{batch_size} \\times T \\times  n_\\text{head} \\times d_\\text{attn} )$\n",
    "        - where $d_\\text{model} $ should be equal to $n_\\text{head} * d_\\text{attn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a [Keras tutorial](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
    "that uses an Encoder and Decoder that are both Transformers\n",
    "- Self attention on the Decoder\n",
    "- Cross attention from the Decoder to the Encoder\n",
    "\n",
    "Here is the relevant code for the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "     def call(self, inputs, encoder_outputs, mask=None):\n",
    "            causal_mask = self.get_causal_attention_mask(inputs)\n",
    "            if mask is not None:\n",
    "                padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "                padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "            attention_output_1 = self.attention_1(\n",
    "                query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "            )\n",
    "            out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "            attention_output_2 = self.attention_2(\n",
    "                query=out_1,\n",
    "                value=encoder_outputs,\n",
    "                key=encoder_outputs,\n",
    "                attention_mask=padding_mask,\n",
    "            )\n",
    "            out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "            proj_output = self.dense_proj(out_2)\n",
    "            return self.layernorm_3(out_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The Decoder input (partially generated English Translation)\n",
    "    - Masked Self Attention on the input via the statement\n",
    "            attention_output_1 = self.attention_1(\n",
    "                    query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "                )\n",
    "        - keys = values = queries = inputs\n",
    "        - **causal masked**: via the option `attention_mask=causal_mask`\n",
    "    - uses Cross attention via the statement\n",
    "    \n",
    "        attention_output_2 = self.attention_2(\n",
    "                query=out_1,\n",
    "                value=encoder_outputs,\n",
    "                key=encoder_outputs,\n",
    "                attention_mask=padding_mask,\n",
    "            )\n",
    "        - query is output of the Self-Attention\n",
    "            - the query is created by self-attention of Decoder input\n",
    "        - keys = values = `encoder_outputs` (sequence of Encoder latent states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code: Encoder-Decoder Transformer\n",
    "\n",
    "Here is the Encoder-Decoder for Spanish to English Translation, using Transformers for both the Encoder and Decoder\n",
    "- Encoder: left-side\n",
    "    - Bottom row: Encoder Spanish Tokens\n",
    "    - Top row: Self-Attention to Spanish tokens\n",
    "- Decoder: right side\n",
    "    - Bottom row: latent representation of English tokens generated so far\n",
    "    - Next row: Decoder Masked Self Attention\n",
    "- Matrix: column $\\tt$\n",
    "    - Attention weight of Decoder output at position $\\tt$ on each of the $\\bar T$ latent representation of the Encoder's Spanish tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Transformer Encoder-Decoder for Spanish to English translation</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\" width=40%>\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above diagram illustrates the difference between\n",
    "- Self Attention in the Encoder (left side)\n",
    "- **Masked** Self Attention in the Decoder (right side)\n",
    "\n",
    "Take a look at the range of positions accessible by the first output position\n",
    "- in the Encoder: **all** Input positions\n",
    "- in the Decoder: **only** the first input\n",
    "\n",
    "Similarly for position $\\tt$\n",
    "- in the Encoder: **all** Input positions\n",
    "- in the Decoder: **only** the prefix of length $\\tt$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We introduced Context Sensitive Memory as the vehicle with which to implement the Attention mechanism.\n",
    "\n",
    "Context Sensitive Memory is similar to a Python dict/hash, but allowing \"soft\" matching.\n",
    "\n",
    "It is easily built using the basic building blocks of Neural Networks, like Fully Connected layers.\n",
    "\n",
    "This is another concrete example of Neural Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We *will not* spend time on the actual code of an Attention layer.\n",
    "\n",
    "If you're interested there are several web articles\n",
    "that do so, for example, [here](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
