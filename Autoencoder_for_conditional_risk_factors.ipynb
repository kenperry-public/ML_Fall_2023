{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbf{R}}\n",
    "\\newcommand{\\r}{\\mathbf{r}}\n",
    "\\newcommand{\\F}{\\mathbf{F}}\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\ntickers}{{n_\\text{tickers}}}\n",
    "\\newcommand{\\ndates}{{n_\\text{dates}}}\n",
    "\\newcommand{\\nfactors}{{n_\\text{factors}}}\n",
    "\\newcommand{\\nchars}{{n_\\text{chars}}}\n",
    "\\newcommand{\\dp}{{(d)}}\n",
    "\\newcommand{\\sp}{{(s)}}\n",
    "\\newcommand{\\Bbeta}{\\mathbf\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor models via Autoencoders\n",
    "\n",
    "A clever way of using Neural Networks to solve a familiar but important problem in Finance\n",
    "was proposed by [Gu, Kelly, and Xiu, 2019](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3335536).\n",
    "\n",
    "It is an extension of the Factor Model framework of Finance, combined with the tools of\n",
    "dimensionality reduction (to find the factors) of Deep Learning: the Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can find [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "for this model as part of the excellent book by [Stefan Jansen](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "- [Github](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "- In order to run the code notebook, you first need to run a notebook for [data preparation](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/05_conditional_autoencoder_for_asset_pricing_data.ipynb)\n",
    "    - This notebook relies on files created by notebooks from earlier chapters of the book\n",
    "    - So, if you want to run the code, you have a lot of preparatory work ahead of you\n",
    "    - Try to take away the ideas and the coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Factor Model review\n",
    "\n",
    "We will begin with a quick review/introduction to Factor Models in Finance.\n",
    "\n",
    "\n",
    "First, some necessary notation:\n",
    "- $\\r^{(d)}_s$: Return of ticker $s$ on day $d$.\n",
    "- $\\hat\\r^{(d)}_s$: approximation of $\\r^{(d)}_s$\n",
    "\n",
    "- $n_\\text{tickers}$: **large** number of tickers\n",
    "- $n_\\text{dates}$: number of dates\n",
    "- $n_\\text{factors}$: **small** number of factors: independent variables (features) in our approximation\n",
    "- Matrix $\\R$ of ticker returns, indexed by *date*\n",
    "    - $\\R: (n_\\text{dates} \\times n_\\text{tickers})$\n",
    "    - $|| \\R^\\dp || = n_\\text{tickers}$\n",
    "        - $\\R^\\dp$ is vector of returns for each of the $\\ntickers$ on date $d$\n",
    "\n",
    "- $\\r$ will denote a vector of single day returns: $\\R^\\dp$ for some date $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notation summary**\n",
    "\n",
    "term &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| meaning\n",
    ":---|:---\n",
    "$s$ | ticker\n",
    "$\\ntickers$ | number of tickers\n",
    "$d$ | date\n",
    "$\\ndates$ | number of dates\n",
    "$\\nchars$   | number of characteristics per ticker\n",
    "$m$ | number of examples\n",
    "    | $m = \\ndates$\n",
    "$i$ | index of example\n",
    "    | There will be one example per date, so we use $i$ and $d$ interchangeably.\n",
    "$ [ \\X^\\ip, \\R^\\ip ]$ | example $i$\n",
    "         | $|| \\X^\\ip || = (\\ntickers \\times  \\nchars )$ \n",
    "         | $|| \\R^\\ip || = \\ntickers$\n",
    "$\\X^\\dp_s$ | vector of ticker $s$'s characteristics on day $d$\n",
    "             | $ || \\X^\\dp_s || = \\nchars$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "The paper actually seeks to predict $\\hat\\r^{(d+1)}_s$ (forward return) rather than approximate\n",
    "the current return $\\hat\\r^\\dp_s$.\n",
    "\n",
    "We will present this as an approximation problem as opposed to a prediction problem for\n",
    "simplicity of presentation (i.e., to include PCA as a model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **factor model** seeks to approximate/explain the return of a *number* of tickers in terms of common \"factors\" $\\F$\n",
    "- $\\F: (\\ndates \\times \\nfactors)$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\R^\\dp_1 & = & \\Bbeta^\\dp_1 \\cdot \\F^\\dp  + \\epsilon_1\\\\\n",
    "\\vdots \\\\\n",
    "\\R^\\dp_\\ntickers & = & \\Bbeta^\\dp_\\ntickers \\cdot \\F^\\dp + \\epsilon_\\ntickers \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "There are several ways to create a factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-defined factors, solve for sensitivities\n",
    "\n",
    "Suppose $\\F$ is given: a matrix of returns of \"factors\" over a range of dates\n",
    "- $\\F^\\dp$ includes the returns of multiple factor tickers\n",
    "    - e.g., market, several industries, large/small cap indices\n",
    "    \n",
    "Solve for $\\Bbeta_s$, for each $s$\n",
    "- $\\ntickers$ separate Linear Regression models\n",
    "- Linear regression for ticker $s$:\n",
    "    - $r_s$ and $\\F$ are time series (length $\\ndates$) of returns for tickers/factors\n",
    "    - Solve for $\\Bbeta_s$\n",
    "        - constant over time\n",
    "        $$\\beta_s^\\dp = \\beta_s$$\n",
    "    - $\\langle \\X^\\dp, \\y^\\dp \\rangle = \\langle  \\F^\\dp, \\r^\\dp_s \\rangle$\n",
    "    \n",
    "$$\n",
    "\\r_s = \\begin{pmatrix}\n",
    " \\r_s^{(1)} \\\\\n",
    " \\r_s^{(2)} \\\\\n",
    "  \\vdots  \\\\\n",
    " \\r_s^{(\\ndates)}\n",
    "  \\end{pmatrix}, \\,\\,\n",
    "\\F = \\begin{pmatrix}\n",
    " \\F_1^{(1)} & \\ldots & \\F_\\nfactors^{(1)}\\\\\n",
    " \\F_1^{(2)} & \\ldots & \\F_\\nfactors^{(2)} \\\\\n",
    "  \\vdots  \\\\\n",
    " \\F_1^{(\\ndates)} & \\ldots & \\F_\\nfactors^{(\\ndates)}\n",
    "  \\end{pmatrix}, \\, \\,\n",
    "\\beta_s = \\begin{pmatrix}\n",
    "\\beta_{s,1} \\\\\n",
    "\\beta_{s,2} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{s, \\nfactors}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "$$\\r_s = \\F * \\beta_s$$\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-defined sensitivities, solve for factors\n",
    "\n",
    "Suppose $\\Bbeta$ is given: \n",
    "- for each ticker $s$: $\\Bbeta_{s, j}$ is the sensitivity of $s$ to $\\F_j$\n",
    "\n",
    "Solve for $\\F^\\dp$ for each $d$\n",
    "\n",
    "- $\\ndates$ separate Linear Regressions\n",
    "- Linear regression for date $d$\n",
    "    - $\\r^\\dp$ and $\\beta^\\dp$ are *cross sections* (width $\\ntickers$) of one day ticker returns/sensitivities\n",
    "    - Solve for $\\F^\\dp$\n",
    "        - constant over tickers\n",
    "        $$\\F_s^\\dp = \\F^\\dp$$\n",
    "    - $\\langle \\X^\\sp, \\y^\\sp \\rangle = \\langle \\Bbeta^\\sp, \\r^\\sp \\rangle$\n",
    " \n",
    "$$\n",
    "\\r^\\dp = \\begin{pmatrix}\n",
    " \\r^\\dp_1 \\\\\n",
    " \\r^\\dp_1 \\\\\n",
    "  \\vdots  \\\\\n",
    " \\r^\\dp_\\ntickers\n",
    "  \\end{pmatrix}, \\,\\,\n",
    "\\F^\\dp = \\begin{pmatrix}\n",
    " \\F^\\dp_1  \\\\\n",
    " \\F^\\dp_2  \\\\\n",
    "  \\vdots  \\\\\n",
    " \\F^\\dp_\\nfactors\n",
    "  \\end{pmatrix}, \\, \\,\n",
    "\\Bbeta = \\begin{pmatrix}\n",
    "\\beta_{1,1}, & \\ldots & \\beta_{1, \\nfactors} \\\\\n",
    "\\beta_{2,1}, & \\ldots  &\\beta_{2, \\nfactors} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{\\ntickers,1}, & \\ldots & \\beta_{\\ntickers, \\nfactors} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br>\n",
    "$$\\r^\\dp = \\Bbeta * \\F^\\dp$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solve for sensitivities and factors: PCA\n",
    "\n",
    "Yet another possibility: solve for $\\Bbeta$ and $\\F$ *simulataneoulsy*.\n",
    "\n",
    "Recall Principal Components\n",
    "- Representing $\\X$ (defined relative to $\\ntickers$ \"standard\" basis vectors) via an *alternate basis* $\\V$\n",
    "$$\\X = \\tilde\\X \\V^T$$\n",
    "\n",
    "In this case, we identify $\\X$ with the returns $\\R$.  Thus, without dimensionality reduction:\n",
    "$$\n",
    "\\R = \\tilde\\R V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\R, \\tilde\\R: (\\ndates \\times n_\\text{tickers}) \\\\\n",
    "\\V^T: (n_\\text{tickers} \\times n_\\text{tickers} ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "PCA seeks to *approximate* $\\R$ with fewer than $\\ntickers$ basis vectors\n",
    "- this is the *dimensionality reduction*\n",
    "- reduce from $n_\\text{tickers}$ dimensions to $n_\\text{factors}$ dimensions\n",
    "\n",
    "$$\\R \\approx \\F \\, \\Bbeta^T$$\n",
    "\n",
    "- $\\F^T: (\\ndates \\times \\nfactors)$\n",
    "    - the \"alternative\" basis: the \"factors\"\n",
    "    - is $\\V$ with columns eliminated b/c of dimensionality reduction\n",
    "- $\\Bbeta^T: (  \\nfactors \\times \\ntickers)$  \n",
    "    - so $\\Bbeta^\\sp$ are sensitivities of $s$ to factors\n",
    "\n",
    "- Solve for $\\F, \\Bbeta$ simultaneously\n",
    "\n",
    "$\\r_s$, the time series of returns of ticker $s$ is approximated by a combination of returns of $\\nfactors$.\n",
    "\n",
    "$$\\r_s^\\dp = \\beta_s * \\F^\\dp$$\n",
    "\n",
    "- $\\beta_s^\\dp$ is constant over time\n",
    "$$\\beta_s^\\dp = \\beta_s$$\n",
    "\n",
    "The daily observation of $\\ntickers$ returns $\\R^\\dp$ is replaced by $\\nfactors$ returns $\\F^\\dp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This paper\n",
    "\n",
    "This paper will create a factor model that\n",
    "- Solve for $\\F, \\Bbeta$ simultaneously\n",
    "    - like PCA\n",
    "- **But** where $\\F$ and $\\Bbeta$ are defined by Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder\n",
    "\n",
    "The paper refers to the model as a kind of Autoencoder.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_vanilla.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's review the topic.\n",
    "- An Autoencoder has two parts: an Encoder and a Decoder\n",
    "- The Encoder maps inputs $\\x^\\ip$, of length $n$\n",
    "- Into a \"latent vectors\" $\\z^\\ip$ of length $n' \\le n$\n",
    "- If $n' \\lt n$, the latent vector is a *bottleneck*\n",
    "    - reduced dimension representation of $\\x^\\ip$\n",
    "- The Decoder maps $\\z^\\ip$ into $\\hat\\x^\\ip$, of length $n$, that is an approximation of $\\x^\\ip$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training examples for an Autoencoder are\n",
    "$$\\langle \\X^\\dp, \\y^\\dp \\rangle = \\langle \\R^\\dp, \\R^\\dp \\rangle$$\n",
    "\n",
    "That is\n",
    "- we want the output for each example to be identical to the input\n",
    "\n",
    "The challenge:\n",
    "- the input is passed through a \"bottleneck\" $\\z$ of lower dimensions than the example length $n$\n",
    "- information is lost\n",
    "- analog: using PCA for dimensionality reduction, but with non-linear operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine that we are given $\\R: (\\ndates \\times \\ntickers)$\n",
    "- timeseries (length $\\ndates$) of returns of $\\ntickers$ tickers\n",
    "\n",
    "Suppose we  map a one day set of returns $\\R^\\dp$ into two separate values\n",
    "- $\\Bbeta^\\dp: (\\ntickers \\times \\nfactors)$ -- the sensitivity of each ticker to each of $\\nfactors$ one day \"factor\" returns\n",
    "- $\\F^\\dp: (\\nfactors \\times 1)$ -- the one day returns of $\\nfactors$ factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to output $\\hat\\R^\\dp$, an approximations of $\\R^\\dp$ such that\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\hat\\R^\\dp & = & \\beta^\\dp * \\F^\\dp \\\\\n",
    "\\hat\\R^\\dp & \\approx & \\R^\\dp \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is the same goal as an Autoencoder but subject to the constraint that $\\hat\\R^\\dp$\n",
    "- is the product of the ticker sensitivities and factor returns\n",
    "\n",
    "The Neural Network *simultaneously* solves for $\\Bbeta^\\dp$ and $\\F^\\dp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This looks somewhat like PCA\n",
    "- **but**, in PCA,  $\\Bbeta$ does not vary by day: it is constant over days\n",
    "- in this model, $\\Bbeta^\\dp$ varies by day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This paper goes one step further than the standard Autoencoder\n",
    "- Inputs $\\X: (\\ndates \\times \\ntickers \\times \\nchars)$ \n",
    "- rather than $\\R: (\\ndates \\times \\ntickers)$\n",
    "\n",
    "Each ticker $s$ on each day $d$, has $\\nchars \\ge 1$ \"characteristic\"\n",
    "- one of them may the daily return $\\R^\\dp$\n",
    "- but may also include a number of other time varying characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The proposed model is a Neural Network with two sub-networks.\n",
    "\n",
    "The *Beta network* computes $\\Bbeta^\\dp_s =  \\text{NN}_\\Bbeta( \\X^\\dp_s ; \\W_\\Bbeta )$\n",
    "- $\\X_s^\\dp$ as input\n",
    "- parameterized by weights $\\W_\\beta$\n",
    "- $\\Bbeta_s^\\dp$ is only a function of $\\X^\\dp_s$, the characteristics of $s$\n",
    "    - and **not** of any other ticker $s' \\ne s$\n",
    "    - $\\Bbeta^\\dp_s$ shares $\\W_\\Bbeta$ across **all** tickers $s'$ and dates $d'$\n",
    "    - contrast this with factor model with fixed factors\n",
    "        - we solve for a separate $\\Bbeta_s$ for each ticker $s$\n",
    "        - via per-ticker timeseries regression\n",
    "    - contrast this with PCA\n",
    "        - $\\beta_s$ is influenced by $\\R_{s'}$ for $s' \\ne s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Factor network* computes $\\F^\\dp = \\text{NN}_\\F(\\R^\\dp, \\W_\\F)$\n",
    "- $\\R^\\dp$ as input (not $\\X^\\dp$ as in the Beta network)\n",
    "- parameterized by weights $\\W_\\F$\n",
    "$\\R^\\dp$ is only a function of $\\R^\\dp$ for date $d$\n",
    "    - and **not** of any other date $d' \\ne d$\n",
    "    - $\\F^\\dp$ shares $\\W_\\F$ across **all** dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This model\n",
    "- has *neither* pre-defined Factors $\\F$ or pre-defined Sensitivities $\\beta$\n",
    "- Simultaneously solve for $\\Bbeta^\\dp_s$ and $\\F^\\dp$\n",
    "\n",
    "Here is a picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder for Conditional Risk Factors</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_for_conditional_risk_factors.png\" width=\"90%\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary of this paper\n",
    "\n",
    "Approximate cross section of daily returns:  $\\hat\\r^\\dp \\approx \\r^\\dp$\n",
    "$$\\r^\\dp \\approx \\hat\\r^\\dp = \\Bbeta^\\dp * \\F^\\dp $$\n",
    "\n",
    "- like an Autoencoder\n",
    "- subject \n",
    "    - to returns as product of sensitivities and factors: $\\hat\\r^\\dp = \\Bbeta^\\dp * \\F^\\dp$\n",
    "    - $\\Bbeta^\\dp_s =  \\text{NN}_\\Bbeta( \\X^\\dp_s ; \\W_\\Bbeta )$\n",
    "    -  $\\F^\\dp = \\text{NN}_\\F(\\R^\\dp, \\W_\\F)$\n",
    "\n",
    "Shapes:\n",
    "- $\\r^\\dp: (n_\\text{tickers} \\times 1)$\n",
    "- $\\Bbeta: (n_\\text{tickers} \\times n_\\text{factors})$\n",
    "- $\\F^\\dp: (n_\\text{factors} \\times 1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Complete Neural Network\n",
    "\n",
    "## Beta (Input) side of network\n",
    "\n",
    "The Beta network $\\text{NN}_\\beta$\n",
    "- input: $\\nchars$ attributes (characteristics) for each of $\\ntickers$ tickers\n",
    "- output: $\\nfactors$ factor sensitivities for each of $\\ntickers$ tickers\n",
    "\n",
    "$$\n",
    "\\text{NN}_\\beta : (\\ntickers \\times \\nchars) \\mapsto (\\ntickers \\times \\nfactors)\n",
    "$$\n",
    "\n",
    "### Input $\\X$\n",
    "\n",
    "$\n",
    "\\X : ( \\ndates \\times \\ntickers \\times \\nchars )\n",
    "$\n",
    "\n",
    "$\\X^\\dp :  (\\ntickers \\times \\nchars)$\n",
    "- Example on date $d$\n",
    "- Consists of $\\ntickers$ tickers, each with $\\nchars$ characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sub Neural network $\\text{NN}_\\Bbeta$\n",
    "\n",
    "$\\text{NN}_\\Bbeta = $ `Dense` $( \\nfactors )( \\X )$\n",
    "- Fully connected network\n",
    "- `Dense` $( \\nfactors )$ computes a function $(\\ntickers \\times \\nchars) \\mapsto (\\ntickers \\times \\nfactors) $\n",
    "- Threads over ticker dimension ([see](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense))\n",
    "    - tickers share same weights across **all** tickers\n",
    "    - single `Dense` $( \\nfactors )$ **not** $\\ntickers$ copies of `Dense`$( \\nfactors )$ with independent weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\W_\\beta: ( \\nfactors \\times \\nchars )$\n",
    "- weights shared across all $d, s$\n",
    "    - $\\W^\\dp_{\\beta, s} = \\W^{(d')}_{\\Bbeta, s'}$ for all $s', d'$\n",
    "    - the transformation of characteristics to beta *independent* of ticker \n",
    "- hence, size of $\\W_\\beta$ is $( \\nfactors \\times \\nchars )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\Bbeta^\\dp = \\text{Dense} \\,( \\nfactors ) ( \\X^\\dp )\n",
    "$$\n",
    "\n",
    "$$ \\Bbeta^\\dp : ( \\ntickers \\times \\nfactors )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factor side of network\n",
    "\n",
    "The Factor network $\\text{NN}_F$\n",
    "- input: vector of ticker returns (one-day)\n",
    "- output: vector of factor returns\n",
    "\n",
    "$$\n",
    "\\text{NN}_\\F: \\ntickers \\mapsto \\nfactors\n",
    "$$\n",
    "\n",
    "### Input $\\R$\n",
    "\n",
    "$\n",
    "\\R : ( \\ndates \\times \\ntickers )\n",
    "$\n",
    "\n",
    "$\\R^\\dp:  (\\ntickers \\times 1)$\n",
    "- Example on date $d$\n",
    "- Consists of returns of $\\ntickers$ tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sub  Neural network $\\text{NN}_\\F$\n",
    "\n",
    "$\\text{NN}_\\F = $ `Dense` $( \\nfactors )$\n",
    "- Fully connected network\n",
    "- `Dense`( $\\nfactors )$ computes a function $\\ntickers \\mapsto \\nfactors$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\W_\\F: (\\nfactors \\times \\ntickers )$\n",
    "- Weights shared across all $d, s$\n",
    "    - $\\W^\\dp_{\\F, s} = \\W^{(d')}_{\\F, s'}$ for all $s', d'$\n",
    "    - the transformation of cross section of ticker returns to Factor returns *independent* of ticker \n",
    "- hence, size of $\\W_\\F$ is $( \\ntickers \\times \\nfactors)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\F^{(d)} = \\text{Dense} \\,( n_\\text{factors} ) ( \\R^{(d)} )\n",
    "$$\n",
    "\n",
    "$$  \\F^{(d)} : \\nfactors$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dot\n",
    "\n",
    "$$\\hat{\\r}^\\dp = \\Bbeta^\\dp \\cdot \\F^\\dp$$\n",
    "\n",
    "Dot product threads over factor dimension\n",
    "- Computes $\\hat\\r^\\dp_s = \\Bbeta_s^\\dp \\cdot \\F^\\dp$ for each $s$\n",
    "    - each $s$ is a row of $\\Bbeta^\\dp$\n",
    "\n",
    "$$ \\hat{\\r}^\\dp :  \\ntickers $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss\n",
    "\n",
    "Let\n",
    "$\\loss^\\dp_{(s)}$ denote error of ticker $s$ on day $d$.\n",
    "$$\n",
    "\\loss^\\dp_{(s)} = \\r^\\dp_s - \\hat{\\r}^\\dp_s\n",
    "$$\n",
    "\n",
    "$\\loss^\\dp$ is the loss, across tickers, on date $d$ (one training example)\n",
    "\n",
    "$$\n",
    "\\loss^\\dp = \\sum_s { \\loss^\\dp_{(s)} }\n",
    "$$\n",
    "\n",
    "The number of examples $m$ equals $\\ndates$\n",
    "\n",
    "So the Total Loss is\n",
    "$$\n",
    "\\loss = \\sum_d { \\loss^\\dp }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Predicting future returns, rather than explaining contemporaneous returns\n",
    "\n",
    "The model is sometimes presented as predicting **day ahead** returns rather than contemporaneous returns.\n",
    "\n",
    "In that case the objective is\n",
    "$$\n",
    "\\hat\\r^\\dp = \\r^{(d + 1)}\n",
    "$$\n",
    "and Loss for a single ticker and date becomes\n",
    "\n",
    "$$\n",
    "\\loss^\\dp_\\sp = \\r^{(d+1)}_s - \\hat{\\r}^\\dp_s\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code\n",
    "\n",
    "The model is built by the function [`make_model`](06_conditional_autoencoder_for_asset_pricing_model.ipynb#Automate-model-generation)\n",
    "\n",
    "    def make_model(hidden_units=8, n_factors=3):\n",
    "        input_beta = Input((n_tickers, n_characteristics), name='input_beta')\n",
    "        input_factor = Input((n_tickers,), name='input_factor')\n",
    "\n",
    "        hidden_layer = Dense(units=hidden_units, activation='relu', name='hidden_layer')(input_beta)\n",
    "        batch_norm = BatchNormalization(name='batch_norm')(hidden_layer)\n",
    "\n",
    "        output_beta = Dense(units=n_factors, name='output_beta')(batch_norm)\n",
    "\n",
    "        output_factor = Dense(units=n_factors, name='output_factor')(input_factor)\n",
    "\n",
    "        output = Dot(axes=(2,1), name='output_layer')([output_beta, output_factor])\n",
    "\n",
    "        model = Model(inputs=[input_beta, input_factor], outputs=output)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is what the model looks like:\n",
    "\n",
    "<img src=\"images/autoencoder_factor_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Highlights\n",
    "- **Two** input layers\n",
    "    - one each for the Beta and Factor networks\n",
    "- The model is passed a **pair** as input\n",
    "    - one input for each side of the network\n",
    "    \n",
    "           Model(inputs=[input_beta, input_factor], outputs=output)\n",
    " \n",
    "     - and is [called](06_conditional_autoencoder_for_asset_pricing_model.ipynb#Train-Model) with a pair\n",
    "     \n",
    "             model.fit([X1_train, X2_train], y_train,\n",
    "                 ...\n",
    "- Loss function: MSE\n",
    "\n",
    "        model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training data\n",
    "\n",
    "    def get_train_valid_data(data, train_idx, val_idx):\n",
    "        train, val = data.iloc[train_idx], data.iloc[val_idx]\n",
    "        X1_train = train.loc[:, characteristics].values.reshape(-1, n_tickers, n_characteristics)\n",
    "        X1_val = val.loc[:, characteristics].values.reshape(-1, n_tickers, n_characteristics)\n",
    "        X2_train = train.loc[:, 'returns'].unstack('ticker')\n",
    "        X2_val = val.loc[:, 'returns'].unstack('ticker')\n",
    "        y_train = train.returns_fwd.unstack('ticker')\n",
    "        y_val = val.returns_fwd.unstack('ticker')\n",
    "        return X1_train, X2_train, y_train, X1_val, X2_val, y_val\n",
    "        \n",
    "- `X1_train`: ticker chacteristics\n",
    "\n",
    "        X1_train = train.loc[:, characteristics].values.reshape(-1, n_tickers, n_characteristics)\n",
    "        \n",
    "- `X2_train`: ticker returns\n",
    "    - Dataframe attribute `returns`\n",
    "\n",
    "            X2_train = train.loc[:, 'returns'].unstack('ticker')\n",
    "        \n",
    "- `y_train`: *forward* ticker returns\n",
    "    - Dataframe attribute `returns_fwd`\n",
    "\n",
    "            y_train = train.returns_fwd.unstack('ticker')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
