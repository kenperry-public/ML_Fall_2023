{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "Our coverage of the Recipe will be rapid and shallow (we use an extremely simple example for illustration).\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "this [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "in order to acquire a more in-depth appreciation of the Recipe.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 1 of the Recipe: [Get the data](Recipe_Overview.ipynb#Get-the-data)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- [Regularization: a way to combat overfitting](Bias_and_Variance.ipynb#Regularization:-reducing-overfitting)\n",
    "    \n",
    "    \n",
    "- [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- Iterative improvement\n",
    "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "        - Regularization\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**The Recipe**\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "the [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "on all steps of the Recipe.\n",
    "\n",
    "\n",
    "**Wrapping up: Intro to Transformations**\n",
    "\n",
    "- [Using pipelines to avoid cheating in cross validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "\n",
    "\n",
    "**Plan**\n",
    "- We introduce a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "**Classification, continued**\n",
    "- [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)\n",
    "- [OHE issue: Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Classsification and Categorical variables wrapup**\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "\n",
    "Time to re-visit, in more depth, several important topics\n",
    "\n",
    "**Plan**\n",
    "- Error Analysis\n",
    "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "    - How Training Loss can be improved\n",
    "- Transformations: we previously studied the mechanics (`sklearn` Pipelines); we now dive into the \"why\" behind creating synthetic features\n",
    "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "- Loss functions\n",
    "    - We look at the mathematical logic behind loss functions\n",
    "\n",
    "**Error Analysis**\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "**Transformations: the \"why\"**\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Part of becoming a better Data Scientist is transforming raw features into more useful synthetic features.\n",
    "\n",
    "Last week, we motivated this by showing the need to make a diverse set of examples (different distributions)\n",
    "more homogeneous.  We show some common Transformations to make this possible, as well as other useful\n",
    "Transformations of the raw features.\n",
    "\n",
    "In an earlier week, we presented the [mechanics](Prepare_data_Overview.ipynb) (how to use `sklearn` to implement transformation Pipelines) of Transformations.  This week, we focus\n",
    "on the necessity (the \"why\").\n",
    "\n",
    "**Transformations: the \"why\" (continued)**\n",
    "\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "        - [Transformations: scaling](Transformations_Scaling.ipynb)\n",
    "        - [Other Transformations](Transformations_Other.ipynb)\n",
    "        \n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "\n",
    "   \n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6   \n",
    "\n",
    "**Plan**\n",
    "- Finish up ensembles\n",
    "- Naive Bayes: a classifier that involves nothing more than counting !\n",
    "- Support Vector Classifiers: a classifier with an interesting twist \n",
    "- Unsupervised Learning: Principal Components\n",
    "\n",
    "**Ensembles (continued)**\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb#Boosting)\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "  \n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "\n",
    "**Plan** \n",
    "- Support Vector Classifiers: continued\n",
    "- Loss functions: mathematical basis\n",
    "- Gradient Descent: how to minimize a Loss function\n",
    "- Recommender Systems: Pseudo SVD\n",
    "- Interpretation: understanding models\n",
    "\n",
    "**Support Vector Classifiers (continued)**\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "\n",
    "Where do the Loss functions of Classical Machine Learning come from ?  We take a brief mathematical\n",
    "detour into Loss functions.\n",
    "\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Descent** \n",
    "\n",
    "Machine Learning is based on minimization of a Loss Function.  Gradient Descent is one algorithm\n",
    "to achieve that.\n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "**Recommender Systems (Pseudo SVD)**\n",
    "\n",
    "How does Amazon/Netflix/etc. recommend products/films to us ?  We describe a method similar to SVD\n",
    "but that is solved using Gradient Descent.\n",
    "\n",
    "This theme of creating a custom Loss Functions and minimizing it via Gradient Descent is a recurring\n",
    "theme in the upcoming Deep Learning second half of the course.\n",
    "\n",
    "- [Recommender Systems](Recommender_Systems.ipynb)\n",
    "- [Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "  \n",
    "**Deeper Dives**\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "\n",
    "\n",
    "## Deep Learning: Headstart\n",
    "\n",
    "### Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Deep Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "# Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "**Review of Midterm Project**\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue our introduction to Neural Networks/Deep Learning\n",
    "\n",
    "- Neural network overview\n",
    "- Neural network: practical\n",
    "- Neural network theory\n",
    "\n",
    "\n",
    "[Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "[Neural Nework Overview (continued)](Neural_Networks_Overview.ipynb#Deep-Learning:-Introduction)\n",
    "\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "    - **Note**\n",
    "        - If you have problems using the `plot_model` function in Keras on your local machine: see [here](Setup_ML_Environment_NYU.ipynb#Tools-for-visualization-of-graphs-(optional)) for a fix.\n",
    "\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/Colab_practical.ipynb)\n",
    "\n",
    "\n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2 Training: continued; Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Plan**\n",
    "- Where do Neural Networks get their power from ?\n",
    "- Automatic computation of gradients\n",
    "- Beyond the feature dimension: Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Practical advice**\n",
    "\n",
    "- Karpathy: [Recipe for training Neural Nets](Karpathy_Recipe_for_training_NN.ipynb)\n",
    "\n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "**How to compute gradients automatically**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "\n",
    "- The Convolutional Neural Network layer type\n",
    "\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_demo.ipynb) (**local machine**) \n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3 Recurrent Neural Networks\n",
    "\n",
    "**Plan**\n",
    "- We continue with our presentation of Convolutional Neural Networks\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "- Introduce a new layer type: Recurrent layers\n",
    "    - Part of our \"sprint\": final layer type\n",
    "    - Will revisit more theoretical issues in subsequent lectures\n",
    "\n",
    "**CNN (continued)**\n",
    "- [CNN: Space and Time (continued)](CNN_Space_and_Time.ipynb#Kernel-size-1)\n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [The dynamics of training](Training_Neural_Networks_Overview.ipynb)\n",
    "    - Effects of changing: activation functions; weight initialization\n",
    "    - initialization and scaling\n",
    "    - dropout\n",
    "    - learning rate schedules\n",
    "    - vanishing/exploding gradients\n",
    "\n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "- [Introduction to Recurrent Neural Network (RNN)](Intro_to_RNN.ipynb)\n",
    "- [Recurrent Neural Network Overview](RNN_Overview.ipynb)\n",
    "- [Using an RNN for Generative AI](RNN_generative.ipynb)\n",
    "    - [LSTM_text_generation from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Keras_examples_LSTM_text_generation.ipynb) (**Colab**)\n",
    "    - [LSTM_text_generation from github](Keras_examples_LSTM_text_generation.ipynb) (**local machine**)\n",
    "\n",
    "**Language Model: a sequence task**\n",
    "- [Language model demo](https://app.inferkit.com/demo)\n",
    "\n",
    "**RNN: Issues**\n",
    "- [Gradients of an RNN](RNN_Gradients.ipynb)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "- [RNN: Visualization](RNN_Visualization.ipynb)\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [RNN: How to deal with long sequences](RNN_Long_Sequences.ipynb)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4 Advanced Recurrent Architectures; Transfer Learning\n",
    "\n",
    "\n",
    "**RNN Issues (continued)**\n",
    "- [RNN Overview (continued)](RNN_Overview.ipynb#Inside-an-RNN-layer)\n",
    "    - [RNN in code](Keras_examples_imdb_cnn.ipynb#Try-an-LSTM-as-a-means-of-obtaining-a-finite-length-representation-of-the-sequence)\n",
    "- [Gradients of an RNN (continued)](RNN_Gradients.ipynb#Calculating-gradients-in-an-RNN)\n",
    "- [RNN: Gradients that Vanish/Explode](RNN_Vanishing_and_exploding_gradients.ipynb)\n",
    "\n",
    "\n",
    "**Review of layer types**\n",
    "- [What layer type to choose](Neural_Network_Layer_Review.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "Sprint is over ! We have covered the basic layer types; time for you to learn by experimenting.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "The \"vanilla\" Recurrent Neural Network (RNN) layer we learned is very much exposed to the problem of vanishing/exploding gradients.\n",
    "\n",
    "We will review the issue and demonstrate a related layer type (the LSTM) designed to mitigate the problem.\n",
    "\n",
    "We  present an extremely useful trick (Transfer Learning) for leveraging the hard work that others have done.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "There are a number of pieces of the LSTM which can appear overwhelming when seen together for the first time.  We will explore these concepts separately before seeing how they are integrated into the LSTM.\n",
    "\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "- [Neural Programming](Neural_Programming.ipynb)\n",
    "\n",
    "**LSTM: An improved RNN**\n",
    "\n",
    "- [Introduction to the LSTM](Intro_to_LSTM.ipynb)\n",
    "- [LSTM Overview](LSTM_Overview.ipynb)\n",
    "\n",
    "**Transfer Learning**\n",
    "\n",
    "Transfer learning allows us to adapt a model trained for one task to be able to solve a new task with a small amount of work.  As models get bigger and bigger, the future of Deep Learning may be one where you use Transfer Learning more than developing your own models from scratch.\n",
    "\n",
    "- [Transfer Learning](Transfer_Learning.ipynb)\n",
    "\n",
    "     - [Transfer Learning example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/TransferLearning_demo.ipynb) (**Colab**)\n",
    "     - [Transfer Learning example from github](TransferLearning_demo.ipynb) (**local machine**)\n",
    "\n",
    "     - [Utility notebook](Dogs_and_Cats_reformat.ipynb)\n",
    "         - Takes the *very large* raw data (from Kaggle) used in the Transfer Learning example\n",
    "         - Creates a much smaller subset, using a different directory structure\n",
    "         - The above notebook uses this reorganized, smaller subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5 Transformers\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We present Attention, a way to enhance the power of RNN's, which is heavily used in a new layer type for sequence processing: the Transformer.  \n",
    "\n",
    "The Transformer layer type is now predominant in the area of Natural Language Processing (NLP).\n",
    "We give a quick introduction but we will revisit it in the module on advanced NLP.\n",
    "\n",
    "\n",
    "**Attention**\n",
    "- [Attention](Intro_to_Attention.ipynb)\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "   \n",
    "**Transformers**\n",
    "- [Transformer](Transformer.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- Transformer\n",
    "    - [Keras example: pre-defined Attention layers](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "        - [notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb)\n",
    "    - [TensorFlow tutorial: implements Attention, positional encoding](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "       - [notebook](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "\n",
    "## NLP\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will make an initial pass on the topic of learning from text: Natural Language Processing.\n",
    "\n",
    "The first pass will use well-established techniques that are relatively easy to follow.\n",
    "\n",
    "We then explore some recent advances that have greatly increased the power of NLP.\n",
    "\n",
    "The Transformer architecture is a key contributor.\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "- [Natural Language Processing Overview](NLP_Overview.ipynb)\n",
    "    - [NLP from github (Colab)](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/Keras_examples_imdb_cnn.ipynb)\n",
    "    - [NLP from github (local machine)](Keras_examples_imdb_cnn.ipynb)\n",
    "   \n",
    "<!--- #include (squad_show.csv) --->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 NLP; Language Models\n",
    "\n",
    "\n",
    "**Learning from text: Deep Learning for Natural Language Processing (NLP)**\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue our study of Natural Language Processing (NLP)\n",
    "\n",
    "- [Natural Language Processing Overview (continued)](NLP_Overview.ipynb#Embeddings)\n",
    "\n",
    "\n",
    "**Evolution of Word representations**\n",
    "- [How to represent a word: syntax](NLP_Tokenization.ipynb)\n",
    "- [How to represent a word: meaning](NLP_Word_Representations.ipynb)\n",
    "\n",
    "## Language Models\n",
    "\n",
    "**Language Models: the future (present ?) of NLP ?**\n",
    "\n",
    "- [Language Models, the future (present ?) of NLP: Review](Review_LLM.ipynb)\n",
    "\n",
    "<!--- #include (squad_show.csv) --->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7 Advanced Topics\n",
    "\n",
    "\n",
    "\n",
    "**Advanced Keras**\n",
    "\n",
    "\n",
    "It turns out that we can't implement these models using the Sequential model of Keras that we have been\n",
    "using all semester:\n",
    "- it's time to give a peek at \n",
    "the Functional model.\n",
    "\n",
    "We look at more advanced Keras techniques that allow us to write complex Loss functions.\n",
    "- Additionally, we will look at other advanced features such as\n",
    "    - Custom training loop\n",
    "    - Cool things you can do with Gradients\n",
    "    - Defining your own layer type\n",
    "\n",
    "We introduce a technique called *Gradient Ascent*. \n",
    "\n",
    "This will be useful both to illustrate Advanced Keras but also to introduce a tool for interpretation.\n",
    "\n",
    "- [Advanced Keras](Keras_Advanced.ipynb)\n",
    "- [Gradient Ascent](Gradient_ascent.ipynb)\n",
    "    - Useful for Interpretaton\n",
    "    \n",
    "**Interpretation**\n",
    "\n",
    "As we've hinted at before: the art of Deep Learning is writing a Loss function that captures the semantics of the problem you want to solve.  \n",
    "\n",
    "-\n",
    "We've spent several weeks learning about Neural Networks.\n",
    "At best, we suggested theories for how they are able to achieve what they do.\n",
    "\n",
    "This week: we will explore methods for testing theories as to how Neural Networks work.\n",
    "\n",
    "\n",
    "**What is a Neural Network really doing ? Interpretation**\n",
    "- [Interpretation: preview](Interpretation_preview.ipynb)\n",
    "- [Introduction to Interpretation of Deep Learning](Intro_to_Interpretation_of_DL.ipynb)\n",
    "- [Interpretation: Simple Methods](Interpretation_of_DL_Simple.ipynb)\n",
    "- [Interpretation: Saliency Maps](Interpretation_of_DL_Deconv.ipynb)\n",
    "- [Interpretation: Gradient Ascent](Interpretation_of_DL_Gradient_Ascent.ipynb)\n",
    "- [Adversarial Examples](Adversarial_Examples.ipynb)\n",
    "\n",
    "**Wrapping up**\n",
    "- [Final thoughts](Deep_Learning_Coda.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/CNN_intro/Ships_in_satellite_images_P2.ipynb)\n",
    "- Data (same as for the Classification assignment)\n",
    "    - please repeat the directions given in that assignment for obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final project; Stock prediction\n",
    "\n",
    " - Assignment notebooks:\n",
    "    - [Stock prediction](assignments/stock_prediction/Final_project_StockPrediction.ipynb)\n",
    "    - [Submission guidelines](assignments/stock_prediction/Final_project.ipynb)\n",
    "   \n",
    " - Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Stock Prediction\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook, submission guidelines notebook\n",
    "        - A directory named `Data/train`\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
