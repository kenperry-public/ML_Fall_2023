{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "Our coverage of the Recipe will be rapid and shallow (we use an extremely simple example for illustration).\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "this [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "in order to acquire a more in-depth appreciation of the Recipe.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 1 of the Recipe: [Get the data](Recipe_Overview.ipynb#Get-the-data)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- [Regularization: a way to combat overfitting](Bias_and_Variance.ipynb#Regularization:-reducing-overfitting)\n",
    "    \n",
    "    \n",
    "- [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- Iterative improvement\n",
    "    - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "        - Regularization\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**The Recipe**\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "the [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "on all steps of the Recipe.\n",
    "\n",
    "\n",
    "**Wrapping up: Intro to Transformations**\n",
    "\n",
    "- [Using pipelines to avoid cheating in cross validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "\n",
    "\n",
    "**Plan**\n",
    "- We introduce a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "**Classification, continued**\n",
    "- [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)\n",
    "- [OHE issue: Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Classsification and Categorical variables wrapup**\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "\n",
    "Time to re-visit, in more depth, several important topics\n",
    "\n",
    "**Plan**\n",
    "- Error Analysis\n",
    "    - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "    - How Training Loss can be improved\n",
    "- Transformations: we previously studied the mechanics (`sklearn` Pipelines); we now dive into the \"why\" behind creating synthetic features\n",
    "    - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "- Loss functions\n",
    "    - We look at the mathematical logic behind loss functions\n",
    "\n",
    "**Error Analysis**\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "**Transformations: the \"why\"**\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Part of becoming a better Data Scientist is transforming raw features into more useful synthetic features.\n",
    "\n",
    "Last week, we motivated this by showing the need to make a diverse set of examples (different distributions)\n",
    "more homogeneous.  We show some common Transformations to make this possible, as well as other useful\n",
    "Transformations of the raw features.\n",
    "\n",
    "In an earlier week, we presented the [mechanics](Prepare_data_Overview.ipynb) (how to use `sklearn` to implement transformation Pipelines) of Transformations.  This week, we focus\n",
    "on the necessity (the \"why\").\n",
    "\n",
    "**Transformations: the \"why\" (continued)**\n",
    "\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "        - [Transformations: scaling](Transformations_Scaling.ipynb)\n",
    "        - [Other Transformations](Transformations_Other.ipynb)\n",
    "        \n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "\n",
    "   \n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6   \n",
    "\n",
    "**Plan**\n",
    "- Finish up ensembles\n",
    "- Naive Bayes: a classifier that involves nothing more than counting !\n",
    "- Support Vector Classifiers: a classifier with an interesting twist \n",
    "- Unsupervised Learning: Principal Components\n",
    "\n",
    "**Ensembles (continued)**\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb#Boosting)\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "  \n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7\n",
    "\n",
    "**Plan** \n",
    "- Support Vector Classifiers: continued\n",
    "- Loss functions: mathematical basis\n",
    "- Gradient Descent: how to minimize a Loss function\n",
    "- Recommender Systems: Pseudo SVD\n",
    "- Interpretation: understanding models\n",
    "\n",
    "**Support Vector Classifiers (continued)**\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Loss functions: mathematical basis**\n",
    "\n",
    "Where do the Loss functions of Classical Machine Learning come from ?  We take a brief mathematical\n",
    "detour into Loss functions.\n",
    "\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Descent** \n",
    "\n",
    "Machine Learning is based on minimization of a Loss Function.  Gradient Descent is one algorithm\n",
    "to achieve that.\n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "**Recommender Systems (Pseudo SVD)**\n",
    "\n",
    "How does Amazon/Netflix/etc. recommend products/films to us ?  We describe a method similar to SVD\n",
    "but that is solved using Gradient Descent.\n",
    "\n",
    "This theme of creating a custom Loss Functions and minimizing it via Gradient Descent is a recurring\n",
    "theme in the upcoming Deep Learning second half of the course.\n",
    "\n",
    "- [Recommender Systems](Recommender_Systems.ipynb)\n",
    "- [Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "  \n",
    "**Deeper Dives**\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "\n",
    "\n",
    "## Deep Learning: Headstart\n",
    "\n",
    "### Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Deep Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "# Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "\n",
    "**Review of Midterm Project**\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue our introduction to Neural Networks/Deep Learning\n",
    "\n",
    "- Neural network overview\n",
    "- Neural network: practical\n",
    "- Neural network theory\n",
    "\n",
    "\n",
    "[Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "[Neural Nework Overview (continued)](Neural_Networks_Overview.ipynb#Deep-Learning:-Introduction)\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/Colab_practical.ipynb)\n",
    "\n",
    "- Karpathy: [Recipe for training Neural Nets](Karpathy_Recipe_for_training_NN.ipynb)\n",
    "\n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2 Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Plan**\n",
    "- Automatic computation of gradients\n",
    "- The Convolutional Neural Network layer type\n",
    "- We start to answer the question posed in our introduction to Neural Networks:\n",
    ">What took so long ?\n",
    "- Why training a Neural Network can be difficult: fine-details of training\n",
    "\n",
    "**How to compute gradients automatically**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2023/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_demo.ipynb) (**local machine**) \n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [Training Neural Networks: fine details (continued)](Training_Neural_Networks_Overview.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
