{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What excites a neuron ?\n",
    "\n",
    "[Feature Visualization: How neural networks build up their understanding of images](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "The inversion process that we described by Deconvolution and Saliency Maps\n",
    "- Is **input dependent** (depends on an example in a dataset)\n",
    "- The Saliency Map for a single (or summary) location at feature map $k$ of layer $\\ll$\n",
    "- Depends on a particular input $\\x^\\ip$ being feed to layer $0$\n",
    "\n",
    "By finding the input examples that \"most excite\" the feature map, we were indirectly able to guess\n",
    "at the feature being recognized by the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now demonstrate a more direct **input independent** approach\n",
    "- Determine the input value (not necessarily an example in a dataset)\n",
    "- That excites (causes large values)\n",
    "- A single location/neuron (or summary) of feature map $k$ of layer $\\ll$\n",
    "\n",
    "By finding the *single input* that most excites a feature map, we may interpret the feature map\n",
    "as attempting to recognize similar inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Ascent: Inverting a Neural Network\n",
    "\n",
    "We have already introduced the notion of computing the *sensitivity* of a feature\n",
    "- At spatial location $\\idxspatial$ of feature map $k$ of layer $\\ll$\n",
    "- To a change in the feature at spatial location $\\idxspatial'$ feature map $k'$ of layer $0$\n",
    "\n",
    "$$\n",
    "\\mathcal{s}_{\\llp, \\idxspatial, k, (0), \\idxspatial', k'} =  \\frac{\\partial \\y_{\\llp, \\idxspatial, k}}{\\partial  \\y_{(0), \\idxspatial', k'}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We used this to define Saliency Maps\n",
    "- Which indicate how much more \"excited\" $\\y_{\\llp, \\idxspatial, k}$ becomes\n",
    "- When we increase the stimulus at layer $0: \\y_{(0), \\idxspatial', k'}$\n",
    "- For a particular input $\\y_{(0)} = \\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also know that Gradient Descent is used \n",
    "- To find the optimal value  $\\W^*$ for the weights $\\W$ that parameterize the layers of a Neural Network\n",
    "- By optimizing (find the minimum) a Loss Function\n",
    "- Using derivatives of the Loss with respect to the weights\n",
    "\n",
    "$$\n",
    "\\W^* = \\argmin{\\W} L(\\hat{\\y},\\y; \\W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens if we *combine* these two ideas:\n",
    "- Find the optimal value for  input $\\x^*$  \n",
    "- By optimizing (maximizing) the value $\\y_{\\llp, \\idxspatial, k}$\n",
    "- Using derivatives of $\\y_{\\llp, \\idxspatial, k}$ with respect to $\\x$ ?\n",
    "\n",
    "$$\n",
    "\\x^* = \\argmax{ \\y_{(0)} = \\x } \\y_{\\llp, \\idxspatial, k}\n",
    "$$\n",
    "\n",
    "(Remember that the value of $\\y_{\\llp, \\idxspatial, k}$ is a function of input $\\x$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is:\n",
    "- We can use Gradient Ascent (rather than Descent, as we are maximizing rather than minimizing the objective)\n",
    "- To find the value $\\x^* = \\y_{(0)}$\n",
    "- That, when used as input to the Neural Network\n",
    "- Maximizes the value of a particular neuron $\\y_{\\llp, \\idxspatial, k}$\n",
    "- Using derivatives\n",
    "$$\n",
    " \\frac{\\partial \\y_{\\llp, \\idxspatial, k}}{\\partial  \\y_{(0), \\idxspatial', k'}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start off by initializing $\\y_{(0)}$ to random noise.\n",
    "- Compute $\\y_{\\llp, \\idxspatial, k}$ on the Forward Pass\n",
    "- Compute $\n",
    " \\frac{\\partial \\y_{\\llp, \\idxspatial, k}}{\\partial  \\y_{(0), \\idxspatial', k'}}\n",
    "$ given the current $\\y_{(0)}$, on the Backward Pass\n",
    "- Move $\\y_{(0)}$ in the direction of the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After some number of epochs, we obtain an $\\x^* = \\y_{(0)}$ that maximizes $\\y_{\\llp, \\idxspatial, k}$.\n",
    "\n",
    "That is: we find the input $\\x^*$ that maximally excites $\\y_{\\llp, \\idxspatial, k}$.\n",
    "\n",
    "We can then interpret $\\y_{\\llp, \\idxspatial, k}$ as looking for the feature\n",
    ">\"Is like $\\x^*$\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we are maximizing a value ($\\y_{\\llp, \\idxspatial, k}$) rather than minimizing one (the Loss)\n",
    "- This method is called *Gradient Ascent*\n",
    "- My multiplying the objective $\\y_{\\llp, \\idxspatial, k}$ by $-1$ we can trivially turns this into a minimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use Gradient Ascent to visualize the inputs that a particular\n",
    "layer in an image classifier (implemented as layers of CNN's) is stimulated by\n",
    "\n",
    "[Visualizing what Convnets learn](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Gradient Ascent is a technique for find the input $\\x^*$\n",
    "that is the \"paradigmatic\" value for a feature at layer $\\ll$\n",
    "\n",
    "It is a simple combination of techniques that we have already learned.\n",
    "\n",
    "You can do many more interesting things with Gradient Ascent\n",
    "- What if your initial guess is not random noise ?\n",
    "- What if you add a constraint on $\\x^*$ ?\n",
    "\n",
    "We will explore these ideas in another lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
