{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic methods for Interpretation\n",
    "\n",
    "We begin our study of Interpretability by presenting simple techniques.\n",
    "\n",
    "Our discussion will be specialized to Neural Networks\n",
    "- Consisting of multiple Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for this specialization is two-fold\n",
    "- They are extremely common for task involving images (something that humans can easily interpret)\n",
    "- The ability of a Convolutional Layer to preserve spatial dimensions\n",
    "- Across Layers\n",
    "- Means its easy to relate features at layer $\\ll$ back to the same spatial location in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's do a quick refresher on the important concepts and notation of Convolutional Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN refresher (notation)\n",
    "\n",
    "(We review concepts from the lecture on Convolutional Neural Networks (CNN)\n",
    "\n",
    "A *feature map* for layer $\\ll$\n",
    "- Is the value of a *single* feature at layer $\\ll$\n",
    "- At *each* spatial location\n",
    "\n",
    "An element of a feature map is the value of the feature at a single spatial location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the feature maps for two layers\n",
    "- Layer $(\\ll-1)$ has three feature maps \n",
    "$$\\y_{(\\ll-1),\\ldots, k} \\text{ for features }1 \\le k \\le 3$$\n",
    "- Layer $\\llp$ has two feature maps\n",
    "$$\\y_{\\llp, \\ldots, k} \\text{ for features }1 \\le k \\le 2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside: Notation reminder**\n",
    "\n",
    "The feature/channel dimension\n",
    "- Appears *last* in the subscripted list of indices (Channel Last convention)\n",
    "- The ellipses ($\\ldots$) signify the variable number of *spatial* dimensions\n",
    "- Thus feature $k$ of layer $\\ll$ is denoted $\\y_{\\llp, \\ldots, k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center><strong>Feature maps</strong></center>\n",
    "    <br>\n",
    "<img src=images/Conv3d_2_feature_maps.png>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each feature map $k$ of layer $\\ll$\n",
    "- Was created by applying a $(f_\\llp \\times f_\\llp \\times n_{(\\ll-1)})$ convolutional kernel $\\kernel_{\\llp,k}$\n",
    "- To layer $(\\ll-1)$ output $\\y_{(\\ll-1)}$\n",
    "\n",
    "We \"slide the kernel\" over all spatial locations  of $\\y_{(\\ll-1)}$ \n",
    "- The Convolutional Layer $\\ll$\n",
    "- Preserves the spatial dimension\n",
    "- But changes the number of features from $n_{(\\ll-1)}$ to $n_\\llp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center><strong>Two layer l feature maps, same spatial location but different output features</strong></center>\n",
    "    <br>\n",
    "<img src=images/Conv3d_2.png>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since a Convolutional layer $\\ll$\n",
    "- Preserves the spatial dimension of its input (layer $(\\ll-1)$ output\n",
    "- Assuming full padding\n",
    "- We can directly relate the spatial location of each feature map\n",
    "- To a spatial location of layer $0$, the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The question we seek to answer:\n",
    "- Can we describe (interpret) the feature being recognized in a single feature map of layer $\\ll$ ?\n",
    "\n",
    "Much of our presentation is based on a very influential paper\n",
    "by [Zeiler and Fergus](https://arxiv.org/abs/1311.2901)\n",
    "- NYU PhD candidate and advisor !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation: The first layer\n",
    "\n",
    "It is relatively easy to understand the features created by the first layer\n",
    "\n",
    "Since feature map $k$ is the result of a dot-product (convolution)\n",
    "- And the dot product is performing a pattern match\n",
    "- Of the pattern given by kernel $\\kernel_{(1),k}$\n",
    "- Against a region of the input\n",
    "- We can interpret layer $1$ as trying to create synthetic features identified by the pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "So all we have to do is examine each kernel to see the pattern for feature $k$ !\n",
    "\n",
    "Here is a visualization of the kernels from the Zeiler and Fergus paper\n",
    "- For 96 individual features\n",
    "- Being computed by the first layer ($1$)\n",
    "- Using a $(7 \\times 7 \\times n_{(0)})$ kernel\n",
    "    - $n_{(0)}$ are the number of input channels\n",
    "\n",
    "Each square is a kernel, whose spatial dimensions are $(7 \\times 7)$ and depth $n_{(0)} = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Layer 1 kernels</strong></center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-112.jpg\", width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"patterns\" being recognized by these kernels seem to represent\n",
    "- Lines, in various orientations\n",
    "- Colors\n",
    "- Shading\n",
    "\n",
    "We interpret Layer $1$ as trying to construct synthetic features representing these simple concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So feature map $k$ of layer $1$ can be interpreted as\n",
    "- Identifying the presence/absence of pattern $\\kernel_{(1),k}$ in input $\\x$\n",
    "- At each spatial location of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Layer 1 Kernel example From  Figure 2**\n",
    "\n",
    "There are kernels looking for \"checkered\" patterns\n",
    "- At row 7, columns 1 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that examining layer $1$ kernels\n",
    "- Is *input independent*\n",
    "- Does not depend on the value of any example $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the first layer: Clustering examples\n",
    "\n",
    "We could try to interpret the kernels of layer $(\\ll \\gt 1)$ but this will be difficult\n",
    "- Layer $\\ll$'s inputs ($\\y_{(\\ll-1)}$) are *synthetic features*, rather than actual inputs\n",
    "- Unless we understand the synthetic features of the earlier layers\n",
    "- We won't be able to interpret the pattern that layer $\\ll$ is matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we can hope to do\n",
    "- Somehow map the representation created by layer $(\\ll >1)$ back to the inputs (layer 0 output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will present several *input dependent* methods \n",
    "- Depend on a particular input example $\\x^\\ip$\n",
    "\n",
    "So the interpretation is only as good as the set of input examples we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The methods will  find *clusters* of examples\n",
    "- That produce a similar feature map\n",
    "- For map $k$ at layer $\\ll$\n",
    "\n",
    "If we can identify a property that is common to all examples in the cluster\n",
    "- We can interpret feature map $k$ of layer $\\ll$ as implementing the feature\n",
    ">\"Is the property present in the input ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maximally Activating Examples\n",
    "\n",
    "Recall that  the *feature map* $k$ of layer $\\ll$\n",
    "- Is matching a pattern (the kernel for $k$)\n",
    "- At each  index $\\idxspatial$ in the spatial dimension\n",
    "- Thus, $\\y^\\ip_{(\\ll), \\idxspatial,k}$ is the intensity of the feature being present at spatial location $\\idxspatial$ of input $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem: there are lots of locations in the spatial dimensions.\n",
    "\n",
    "Rather than examining all locations, we\n",
    " we can *summarize* whether the feature exists *anywhere* in example $i$.\n",
    "- i.e, we attempt to interpret what feature map $k$ is recognizing in general, rather than at specific location $\\idxspatial$\n",
    "\n",
    "For example, using \"max\" for summarization\n",
    "- We can identify the *value* $\\summaxact^\\ip_{\\llp,k}$ of the strongest activation\n",
    "- Without identifying its exact location\n",
    "\n",
    "$$\n",
    "\\summaxact^\\ip_{\\llp,k} = \\max{ \\idxspatial } \\y^\\ip_{(\\ll), \\idxspatial,k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By sorting examples on $\\summaxact^\\ip_{\\llp,k}$\n",
    "- We identify a cluster of examples\n",
    "- That are most identified with the feature\n",
    "\n",
    "These examples with largest $\\summaxact^\\ip_{\\llp,k}$ are the *Maximally Activating Examples* for feature $k$ of layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we can identify a common property among the examples with largest $\\summaxact^\\ip_{\\llp,k}$\n",
    "- We can interpret feature map $k$ of layer $\\ll$ as implementing the feature\n",
    ">\"Is the property present in the input ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally\n",
    "- Let $\\text{MaxAct}_{\\llp,k} = [ i_1, \\ldots, i_m ]$ be the permutation of example indices, i.e., $[ i | 1 \\le i \\le m]$\n",
    "- That sorts $\\summaxact^\\ip_{\\llp,k}$ in ascending order\n",
    "$$\n",
    "\\summaxact^{(i_1)}_{\\llp,k} \\ge \\summaxact^{(i_2)}_{\\llp,k} \\ge \\ldots \\ge \\summaxact^{(i_m)}_{\\llp,k}$\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this way we can try to interpret the feature map of each layer.\n",
    "\n",
    "Applying this technique to \n",
    "layer $L$ (the \"head\", a Classifier in the case of MNIST) is particularly useful\n",
    "- We can identify the examples most/least strongly identified with the concept of each digit\n",
    "- Because $\\y^\\ip_{(L),k}$ is the *probability* that example $i$ is digit $k \\in  \\{ 0, \\ldots, 9 \\}$\n",
    "\n",
    "Here are examples of the digit \"8\" that maximally/minimally activate the classifier's \"8\" output $\\y_{(L),8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>MNIST CNN maximally activating 8's</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/best_worst_8.png\"></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interesting !  Do we have a problem with certain 8's ?\n",
    "\n",
    "Much lower probability when\n",
    "- 8 is thin versus thick\n",
    "- tilted left versus right\n",
    "\n",
    "So although our goal was interpretation, this technique may be useful for Error Analysis as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Occlusion\n",
    "\n",
    "Maximally activating inputs are very coarse: they identify concepts at the level of entire input.\n",
    "    \n",
    "But, it's reasonable to suspect that some elements of the input are more important to the concept than others.\n",
    "\n",
    "In particular, a CNN has a \"receptive field\" which defines the input elements that contribute to the layer output.\n",
    "\n",
    "Close to the input layer, the receptive field is narrow so its clear that the \"features\" being identified are small in span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Occlusion is one way of identifying the elements of the input layer that most affect the latent\n",
    "representation.  \n",
    "\n",
    "We will describe this in terms of a 2D input, but we can generalize.\n",
    "\n",
    "Let\n",
    "- $\\y_{\\llp,j}^\\ip$ denote the response of feature $\\y_{\\llp,j}$ to input $\\x^\\ip$.\n",
    "- Place an occluding square over some portion of input $\\x^\\ip$ and measure the change in $\\y_{\\llp,j}$\n",
    "- Do this for each location in input $\\x^\\ip$ and create a \"heat map\" of changes in response $\\y_{\\llp,j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use occlusion to see how images of the digit \"8\" are recognized\n",
    "- Perhaps: by the two \"donut\" holes and \"pinched waist\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <th><center>Occlusion: Relative decrease in probability of being \"8\"</center></th>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/occlude_8_4_1.png\" width=600>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not what we expected !  \n",
    "\n",
    "The labels above each image is the reduction in probability of correct classification when the\n",
    "image is occluded relative to the un-occluded imaged.\n",
    "\n",
    "The mere presence of the square changes the classification probability\n",
    "greatly, even when we are not blocking the \"waist\" of the 8.\n",
    "\n",
    "**Curiousity**\n",
    "\n",
    "Occlusion reduces the probability of correct classification by 81% **for all but one** occluded image\n",
    "- Third row from bottom, third column\n",
    "- Is it because\n",
    "    - the occluding square is attached to the body ?\n",
    "    - there are no images in the training set with bright pixels in the location of the occluding square ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the change in response of a single feature map in layer 5 of an image classifier (Zeiler and Fergus).\n",
    "\n",
    "The chosen feature map is the one with the highest activation level in the layer.\n",
    "\n",
    "You can see that it is responding to \"faces\"\n",
    "- Occluding each of the two faces causes a \"drop in temperature\" (lower intensity)\n",
    "- \"Hot\": red; \"Cold\": blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Activation of one filter at layer 5</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-148.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Zeiler and Fergus also measured the change in activation of $\\y_{(L),j}^\\ip$, the logit corresponding to the correct\n",
    "class (\"Afghan Hound\").\n",
    "- \"Hot\" colors: increase in intensity of the \"Afghan Hound\" logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "      <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Change in logit for \"Afghan hound\"</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-145.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Occluding the dog causes a big drop in probability of correct classification\n",
    "\n",
    "But occluding each face increases the probability of correct classification !\n",
    "- Perhaps the presence of a face is suggestive of an alternative class\n",
    "- Even though \"face\" is not itself a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We began our quest for understanding how Neural Networks work with simple techniques.\n",
    "\n",
    "The first technique\n",
    "- Find clusters of example\n",
    "- Created by a particular feature map\n",
    "- Relate a human-observable common property of the cluster\n",
    "- To the feature that the feature map is attempting to recognize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas clustering identifies groups of examples, the second technique tries to find *sub-regions* of the examples\n",
    "\n",
    "Occlusion measures the change in response of a feature map summary (or single neuron)\n",
    "- When a sub-region of the input is visible\n",
    "- Versus when it is not visible\n",
    "\n",
    "The interpretation that arises is that the feature map is attempting to recognize a property in a narrow area.\n",
    "\n",
    "So, beyond clustering, it is attempting to *localize* the spatial location of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
