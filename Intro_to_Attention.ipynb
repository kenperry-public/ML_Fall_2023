{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention: motivation\n",
    "\n",
    "Let us consider a task familiar to all of us who have taken standardized exams: Question Answering.\n",
    "\n",
    "Input consists of two pieces of text\n",
    "- a paragraph (called the *Context*)\n",
    "- a Question whose answer can be found in the paragraph\n",
    "\n",
    "Output is a piece of text that answers the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\x = \\;\n",
    "\\begin{Bmatrix}\\\\\n",
    "\\text{Context:} & \\text{The FRE Dept offers many Spring classes.  The students are great. ...} \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Perry taught them Machine Learning. The students ...}, \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Blecherman led a class in ...} \\\\\n",
    "& \\vdots \\\\\n",
    "\\text{Question:} & \\text{What did Professor Perry do ?} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "<br><br><br>\n",
    "$\n",
    "\\y = \\;\n",
    "\\begin{array} \\\\\n",
    "\\text{Answer:} & \\text{He taught them Machine Learning}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us hypothesize how a model might learn to solve this task\n",
    "- it is only an hypothesis: we don't really know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model might have generalized\n",
    "- from seeing many training examples of disparate questions and their answers\n",
    "- that there is a parameterized *template* for both the Question and the Answer\n",
    "\n",
    "Question Template\n",
    "\n",
    "> What did Professor `<PROPER NOUN>` teach in the Spring ?\n",
    "\n",
    "Answer Template:\n",
    "```\n",
    "<PRONOUN> <VERB> <INIDRECT OBJECT> <OBJECT>\n",
    "```\n",
    "\n",
    "where `<PROPER NOUNE>, <PRONOUN>, <VERB>`, etc. are *pattern place-holders* parameters.\n",
    "\n",
    "By using a parameterized template, the model captures\n",
    "- commonality\n",
    "- in many different types of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to produce the answer the model needs to\n",
    "- Generate the tokens of the Answer Template in order\n",
    "- Substituting in concrete values for the place-holders\n",
    "    - by performing a Lookup in the Context in order to obtain these values\n",
    "\n",
    "We will examine how the Lookup might be performed\n",
    "- first: by using mechanisms that we have already studied\n",
    "- subsequently: via a new mechanism called *Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an RNN without Attention\n",
    "\n",
    "## Encoder-Decoder architecture: review\n",
    "\n",
    "For the Question Answering task\n",
    "- both the Input and Output are sequences\n",
    "- thus, the task is a Sequence to Sequence task\n",
    "    - just like: Language Translation\n",
    "\n",
    "We learned that Recurrent architectures are best-suited for processing sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These architectures\n",
    "- operate in a \"loop\"\n",
    "    - processing one Input or Output token at a time\n",
    "- utilize **memory** (latent state)\n",
    "    - necessary because Input/Output sequence lengths are unbounded\n",
    "    - after processing the token at position $\\tt$\n",
    "        - the latent state is finite representation of the prefix of the sequence of length $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The use of latent state/memory evolved over the models we studied\n",
    "- RNN\n",
    "    - latent state encodes\n",
    "        - input representation\n",
    "        - \"control\" state\n",
    "            - guiding how the model processes the data: state transitions\n",
    "- LSTM\n",
    "    - latent state partitioned into\n",
    "        - Short Term memory: control state\n",
    "        - Long Term memory\n",
    "        \n",
    "Both these models processed the input sequence **once**\n",
    "- so input-specific representation needs to be part of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common architecture for Sequence to Sequence tasks is the Encoder-Decoder:\n",
    "- The Encoder is an RNN\n",
    "    - Acts on input sequence $[\\x_{(1)} \\dots \\x_{(\\bar{T})}]$\n",
    "    - Producing a sequence of latent states $[ \\bar{\\h}_{(1)}, \\dots, \\bar{\\h}_{(\\bar{T})} ]$\n",
    "        - latent state $\\bar{\\h}_\\tt$ is a summary of $[\\x_{(1)} \\dots \\x_\\tp ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder\n",
    "- Acts on the *final* Encoder latent state $\\bar{\\h}_{(\\bar{T})}$\n",
    "    - which summarizes the entire input sequence $\\x$\n",
    "    - Producing a sequence of latent states $[ \\h_{(1)}, \\dots, \\h_{(T)} ]$\n",
    "        - latent state $\\h_\\tp$ is response for generating output token $\\hat{\\y}_\\tp$\n",
    "    - Thus outputting a sequence  $[ \\hat{\\y}_{(1)}, \\dots, \\hat{\\y}_{(T)} ]$\n",
    "- Often feeding step $\\tt$ output $\\hat{\\y}_\\tp$ as Encoder input at step $(\\tt+1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder output $\\hat\\y_\\tp$\n",
    "\n",
    "The simplest RNN (corresponding to our diagrams) use the latent state $\\h_\\tp$ as the output $\\hat\\y_\\tp$\n",
    "$$\n",
    "\\hat\\y_\\tp = \\h_\\tp\n",
    "$$\n",
    "\n",
    "It is easy to add another NN to transform $\\h_\\tp$ into a $\\hat\\y_\\tp$ that is different.\n",
    "\n",
    "- We can add a NN to the Decoder RNN that implements a function $D$ that transforms the latent state into an output.\n",
    "\n",
    "$$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "For clarity: we will omit this additional NN from our diagrams until it becomes necessary\n",
    "\n",
    "Here is what the additional NN looks like:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_no_attention.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How does the Decoder perform Lookup (without Attention) ?\n",
    "\n",
    "Suppose the Decoder has already output \n",
    "$$\\hat\\y_{([1:3])} = \\text{He taught them}$$\n",
    "\n",
    "It must subsequently output\n",
    "$$\n",
    "\\hat\\y_{([4:5])} = \\text{Machine Learning}$$\n",
    "\n",
    "In order to do this\n",
    "- it must Lookup \"Machine Learning\" in the Context, resulting in\n",
    "$$\\begin{array} \\\\\n",
    "D( \\h_{(4)}) & = & \\text{Machine} \\\\\n",
    "D( \\h_{(5)}) & = & \\text{Learning} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But  $D$ is conditioned on the single input $\\h_\\tp$.\n",
    "\n",
    "Thus, in order for $D( \\h_{(4)} )$ to be equal to \"Machine\"\n",
    "- this information must somehow be encoded in $\\h_{(4)}$\n",
    "\n",
    "\n",
    "How did it get there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All \"knowledge\" from the Context must be transfered from Encoder to Decoder\n",
    "- through final Encoder state $\\bar \\h_{(\\bar T)}$\n",
    "- which in turn was encoded in all Encoder states $\\bar \\h_{({\\bar \\tt}')} \\text{ for } \\tt' \\ge \\bar p$\n",
    "    - where $\\bar p$ is the position withing sequence $\\x$ of the word \"Machine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can hypothesize that the final Encoder latent state $\\bar\\h_{\\bar T}$\n",
    "- encodes a Dictionary (key/value pairs)\n",
    "- mapping Place-holder names to Concrete values\n",
    "- the dictionary is built incrementally by prior latent states of the Encoder\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Answering questions using Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_1.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This dictionary is passed to the Encoder\n",
    "- and must be carried forward by the Decoder\n",
    "- through Decoder states $[ \\h_{(1)}, \\dots, \\h_{(4)} ]$\n",
    "- in order to make the dictionary available to subsequent latent states of the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We further hypothesize that the Decoder\n",
    "- performs Lookups \n",
    "- by using the Decoder latent state $\\h_\\tp$\n",
    "    - as a *query* that matches against the keys of the Dictionary\n",
    "    - in order to obtain the Concrete value required to produced output token at position $\\tt$\n",
    "    $$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Query performing a Lookup in the Dictionary</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_2.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture describing this hypothetical functioning.\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder without Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Connecting the Encoder and Decoder through the \"bottleneck\" of $\\bar \\h_{(\\bar T)}$ thus burdens the\n",
    "- Encoder: passing knowledge forward to the bottleneck\n",
    "- Decoder: passing knowledge from the bottleneck\n",
    "\n",
    "This results in an inefficient use of the model's weights\n",
    "- In addition to\n",
    "    - the Encoder and Decoder allocating some of the model weights for \"control\"\n",
    "    - guiding the loop that processes the Input, or generates the output positions in the template\n",
    "- It must **also** allocate some model weights for \"knowledge storage\"\n",
    "    - in order to Lookup the concrete value corresponding to a place-holder in the Output template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "[paper that introduced Attention](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<br><br>\n",
    "The flaw in the Encoder-Decoder without Attention is \n",
    "- the input $\\x$ is processed *only once*\n",
    "- by the Encoder\n",
    "- which has to summarize it in $\\bar{\\h}_{(\\bar T)}$\n",
    "\n",
    "We will introduce a mechanism called *Attention*\n",
    "- that allows the input sequence to be *re-visited* at each time step of Output generation\n",
    "\n",
    "This will result in a cleaner separation between control memory and input memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention allows the Decoder\n",
    "- to directly access all of the Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "- at each time step of the Decoder\n",
    "\n",
    "Thus, there is no need\n",
    "- for an Encoder to create a full dictionary as the final Encoder latent state $\\bar\\h_{(\\bar T)}$\n",
    "- for the Decoder to keep the dictionary in all it's latent states $\\h_{(1)} \\dots \\h_{(T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture of an Encoder/Decoder augmented with Attention\n",
    "- we have add an additional box to the diagram for the NN that implements the function $D$\n",
    "    - that maps $\\h_\\tp$ to $\\y_\\tp$\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the final Encoder latent state $\\bar\\h_{(\\bar T)}$ is **no longer**  connected to the Decoder.\n",
    "\n",
    "What is going on inside the \"box\" implementing function $D$ that we added at each time step ?\n",
    "\n",
    "The box's input at step $\\tt$\n",
    "- the Decoder latent state $\\h_\\tp$\n",
    "- the collection of Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "    - the red box in the above diagram\n",
    "\n",
    "That is, it is computing a $\\y_\\tp$ that is a function of both $\\h_\\tp$ and $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "\n",
    "$$\n",
    "\\hat\\y_\\tp = D( \\h_\\tp,  [ \\bar\\h_{(1)} \\dots \\h_{(\\bar{T})} ])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the inner workings of the NN for $D$:\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation with attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inside the box:\n",
    "- the Decoder latent state $\\h_\\tp$ is used as a *query*\n",
    "- which is matched against each of the Encoder latent states\n",
    "- resulting in one Encoder latent state being chosen as $\\mathbf{c}_\\tp$\n",
    "\n",
    "The chosen Encoder latent state $\\mathbf{c}_\\tp$ and Decoder latent state $\\h_\\tp$\n",
    "- are input to another Neural Network\n",
    "- which produces output $\\hat\\y_\\tp$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Essentially we change  $D$ so that it is conditioned on\n",
    "- $\\h_\\tp$: the query (Decoder state)\n",
    "- **and** summary of the Context $\\bar \\h_{([1:\\bar T])}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "The \"Choose\" box implements an *Attention* mechanism allows the Decoder\n",
    "- to **attend to** the part of Input $\\x$ (represented via some Encoder latent state $\\bar\\h_{(\\bar\\tt)}$)\n",
    "- that is *relevant* for producing $\\hat \\y_\\tp$\n",
    "- exactly when it is needed\n",
    "\n",
    "This seems very natural to a human\n",
    "- rather than memorizing details (e.g., the big dictionary $\\bar\\h_{(\\bar T)}$ in the architecture without Attention)\n",
    "- we refer back to the context\n",
    "- focusing of only the part that is immediately needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A big advantage of this approach:\n",
    "- the weights of the Decoder are solely for \"control\" (e.g., creating the output according to the Answer template)\n",
    "- and not for logic to copy the \"knowledge\" (dictionary) from latent state to latent state of the Decoder\n",
    "- potentially making it easier to train (by more efficient use of the limited number of weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The discussion of the **implementation** of Attention will be deferred to a\n",
    "later module [Attention lookup](Attention_Lookup.ipynb).\n",
    "\n",
    "For now, think of the \"Choose\" box as a Context Sensitive Memory (as described in the module on [Neural Programming](Neural_Programming.ipynb#Soft-Lookup))\n",
    "- Like a Python `dict`\n",
    "    - Collection of key/value pairs: $\\langle \\bar\\h_{(\\bar \\tt)}, \\bar\\h_{(\\bar \\tt)} \\rangle$\n",
    "    - Key is equal to value; they are latent states of the Encoder\n",
    "- But with *soft* lookup\n",
    "    - The current Decoder state $\\h_\\tp$ is presented to the CSM \n",
    "        - Called the *query*\n",
    "        - Is matched across each key of the dict (i.e., a latent state $\\bar \\h_{(\\bar \\tt)}$)\n",
    "    - The CSM returns an approximate match of the query to a *key* of the `dict`\n",
    "        - The distance between the query and each key in the CSM is computed\n",
    "        - The Soft Lookup returns a *weighted* (by inverse distance) sum of the *values* in the CSM `dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Have we seen this before ?\n",
    "\n",
    "If you recall the architecture of the LSTM\n",
    "- *short term* (control) memory \n",
    "- was separated from *long term* memory\n",
    "- elements of long term memory are moved to short term memory *as needed*\n",
    "\n",
    "This is partly similar to the advantages of Attention.\n",
    "\n",
    "But\n",
    "- all factual information from input $\\x$ has to flow through the bottleneck $\\bar \\h_{(\\bar T)}$ of the Encoder output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualizing Attention\n",
    "\n",
    "We can illustrate the behavior of Neural Networks that have been augmented with Attention through diagrams.\n",
    "- at a particular output position $\\tt$\n",
    "- we can display the amount of \"attention\"\n",
    "- that each position in the input receives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention can be used to create a Context Sensitive Encoding of words\n",
    "- The meaning of a word may change depending on the rest of the sentence\n",
    "\n",
    "We can illustrate this with an example: how the meaning of the word \"it\" changes\n",
    "- The thickness of the blue line indicates the attention weight that is given in processing the word \"it\".\n",
    "\n",
    "<img src=https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of the recent advances in NLP may be attributed to these improved, context sensitive embeddings.\n",
    "\n",
    "We note that simple Word Embeddings\n",
    "- also capture \"meaning\"\n",
    "- but are *not* sensitive to context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Entailment: Does the \"hypothesis\" logically follow from the \"premise\"**\n",
    "\n",
    "<br>\n",
    "<center><strong>Attention: Entailment</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_visualization_Entailment.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Does the Premise logically entail the Hypothesis.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1509.06664.pdf#page=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Date normalization example**\n",
    "- Source: Dates in free-form: \"Saturday 09 May 2018\"\n",
    "- Target: Dates in normalized form: \"2018-05-09\"\n",
    "\n",
    "[link](https://github.com/datalogue/keras-attention#example-visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Image captioning example**\n",
    "- Source: Image\n",
    "- Target: Caption: \"A woman is throwing a **frisbee** in a park.\"\n",
    "- Attending over *pixels* **not** sequence\n",
    "\n",
    "<center><strong>Visual attention</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/shat_-002-027.jpg\"></td>\n",
    "        <td><img src=\"images/shat_-002-028.jpg\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>A woman is throwing a <strong>frisbee</strong> in a park.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/pdf/1502.03044.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-attention\n",
    "\n",
    "We have illustrated the benefit of enabling the Decoder to attend to the Encoder.\n",
    "\n",
    "This form of attention is called *Cross Attention*.\n",
    "\n",
    "But we can further simplify the Decoder control by enabling it, when generating $\\hat \\y_\\tp$\n",
    "- to attend to all previously generated outputs $\\hat \\y_{([1:\\tt-1])}$\n",
    "\n",
    "This form of attention if called *Self Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, suppose the Decoder is generating a long sentence\n",
    "- in many languages, there needs to be agreement between the gender/plurality of a subject and verbs\n",
    "- Self attention enables the Decoder to refer back to the previously generated subject of the sentence\n",
    "- when generating the verb for each subsequent output position\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common in an Encoder-Decoder architecture to have both\n",
    "- Cross Attention from Decoder to Encoder\n",
    "- Self Attention from Decoder to Decoder\n",
    "\n",
    "We will see both forms used in the Transformer.\n",
    "\n",
    "These mechanisms are attending to different sequences (Encoder states or Decoder outputs).\n",
    "\n",
    "We will henceforth use the term *sequence being attended to* as a general term\n",
    "- instead of specifically referring to the part of the network that produced it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Masked attention\n",
    "\n",
    "As presented, the Attention mechanism can refer to an entire sequence\n",
    "- e.g., the sequence of Encoder latent states\n",
    "\n",
    "It is sometimes desirable to *limit* what may be attended to.\n",
    "\n",
    "For example, consider a decision at time $\\tt$ that may depend *only on the past*\n",
    "- positions $\\tt' \\lt \\tt$\n",
    "- for example, a trading decision at time $\\tt$ may depend only on *prior* information\n",
    "    - typical of sequences that are timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Restricting attention to the past is called *Causal Attention*.\n",
    "- the next output depends only on things that could have caused it (the past), not the future\n",
    "\n",
    "There is a mechanism to restrict what may be attended to in a general way\n",
    "- create a \"mask\"\n",
    "- a bit vector for each position of the sequence being attended to\n",
    "- such that attention is limited to positions where the mask element if True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *Masked Attention*.\n",
    "\n",
    "It is frequently used to enable a Decoder, when predicting output $\\hat \\y_\\tp$\n",
    "- to attend to **previously** generate outputs $\\hat \\y_{[1:\\tt-1])}$\n",
    "- but not **future** outputs $\\hat\\y_{(\\tt')}$ for $\\tt' \\ge \\tt$\n",
    "\n",
    "When used in this manner, we refer to the behavior as *Masked Self Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "You may wonder how it is even practically possible for a Decoder to refer to the future.\n",
    "\n",
    "When using *Teacher Forcing* for **training**\n",
    "- the Decoder does not use the *predicted* target sequence $\\hat \\y_{(1:T)}$\n",
    "- the Decoder uses the *actual* target sequence $\\y_{(1:T)}$\n",
    "    - hence, \"future\" positions $\\tt' \\ge \\tt$ are available\n",
    "- this prevents a single mis-prediction at position $\\tt$ from cascading and ruining all future output\n",
    "    - facilitates training\n",
    "- at inference time: the Decoder works on the *predicted* Target sequence.\n",
    "\n",
    "In the diagram below, we illustrate (lower right) how the Decoder input changes between Training and Test/Inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing) + inference: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-head attention: two heads are better than one\n",
    "\n",
    "Perhaps when generating the output for position $\\tt$ of the output sequence\n",
    "- we need to attend to *more than one* position of the sequence being attended to\n",
    "    - need to know both gender and plurality of subject\n",
    "- that is: we want an Attention layer to output multiple items.\n",
    "\n",
    "We can attend to $n$ positions\n",
    "- by creating $n$ separate Attention mechanisms\n",
    "- each one called a *head*\n",
    "\n",
    "This behavior is referred to as *Multi-head attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of behavior is common to many layer types in a Neural Network\n",
    "- a Dense layer $l$ may produce a vector $\\y_\\llp$ where $n_\\llp \\gt 1$\n",
    "- a Convolutional layer $l$ may produce outputs (for each spatial location) for many channels\n",
    "\n",
    "We have referred to this as layer $\\ll$ producing $n_\\llp$ *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be natural for an Attention layer to output many \"features\" to enable attention to many positions.\n",
    "\n",
    "In practice, this is sometimes (always ?) not done\n",
    "- Model architectures (e.g., the Transformer) are simplified when the inputs/outputs of each sub-component\n",
    "- have the same length\n",
    "- often denoted as $d$ of $d_\\text{model}$ in the Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When a Transformer needs to attend to $n$ positions\n",
    "- it uses $n$ Attention heads\n",
    "- each outputting a vector of length $\\frac{d}{n}$\n",
    "- which are concatenated together to produce a single output of length $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we have $n$ heads\n",
    "- Rather than having one Attention head operating on vectors of length $d$\n",
    "    - producing an output of length $d$ (weighted sum of values in the CSM)\n",
    "- We create $n$ Attention heads operating on vectors (keys, values, queries) of length $d \\over n$.\n",
    "    - Output of these smaller heads are values, and hence also of length $d \\over n$\n",
    "- The final output concatenates these $n$ outputs into a single output of length $d$\n",
    "    - identical in length to the single head\n",
    "- we project each of these length $d$ vector into vectors of length $d \\over n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The picture shows $n$ Attention heads.\n",
    "\n",
    "Note that each head is working on vectors of length $\\frac{d}{n}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "Details are deferred to the module [Attention lookup](Attention_Lookup.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each head $j$ uniquely transforms the query $\\h_\\tp$ and the key/value pairs $\\bar{\\h}_{(1)} \\ldots \\bar{\\h}_{(\\bar{T})}$ being queried.\n",
    "- into $\\h^{(j)}_\\tp$ and the key/value pairs $\\bar{\\h}^{(j)}_{(1)} \\ldots \\bar{\\h}^{(j)}_{(\\bar{T})}$\n",
    "- Such that each head attends to a separate item\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "There is a new model (the Transformer) that processes sequences much faster than RNN's.\n",
    "\n",
    "It is an Encoder/Decoder architecture that uses multiple forms of Attention\n",
    "- Self Attention in the Encoder\n",
    "    - to tell the Encoder the relevant parts of the input sequence $\\x$ to attend to\n",
    "- Decoder/Encoder attention\n",
    "    - to tell the Decoder which Encoder state $\\bar{\\h}_{(\\tt')}$ to attend to when outputting $\\y_\\tp$\n",
    "- Masked Self-Attention in the Decoder\n",
    "    - to prevent the Decoder from looking ahead into inputs that have not yet been generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We recognized that the Decoder function responsible for generating Decoder output $\\hat{\\y}_\\tp$\n",
    "$$\n",
    "\\hat{\\y}_\\tp = D( \\h_\\tp; \\mathbf{s})\n",
    "$$\n",
    "\n",
    "was quite rigid when it ignored argument $\\mathbf{s}$.\n",
    "\n",
    "This rigidity forced Decoder latent state $\\h_\\tp$ to assume the additional responsibility of including Encoder context.\n",
    "\n",
    "Attention was presented as a way to obtain Encoder context through argument $\\mathbf{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
