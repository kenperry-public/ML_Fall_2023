{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Models\n",
    "\n",
    "A *Language Model* is an instance of the \"predict the next\" paradigm where\n",
    "- given a sequence of words\n",
    "- we try to predict the next word\n",
    "\n",
    "Recall the architecture to solve \"predict the next word\" and data preparation\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "<tr>\n",
    "    <center><strong>Language Modeling task</strong></center>\n",
    "</tr>\n",
    "    <br>\n",
    "<tr>\n",
    "    <th><center>Architecture</center></th>\n",
    "    <th><center>Data preparation</center></th>\n",
    "    </tr>\n",
    "<tr>\n",
    "    <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=70%></td>\n",
    "    <td><center>$\\y = \\y_{(1)}, \\ldots, \\y_{(T)}$</center>\n",
    "        <br><br><br>\n",
    "\\begin{array} \\\\\n",
    "      i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "      \\hline \\\\\n",
    "      1 & \\y_{(0) }  & \\y_{(1)} \\\\\n",
    "      2 & \\y_{([0:1]) }  & \\y_{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      \\tt & \\y_{([1:\\tt-1]) }  & \\y_\\tp \\\\\n",
    "      \\vdots \\\\\n",
    "      T & \\y_{([1:T-1]) }  & \\y_{(T)} \\\\\n",
    "\\end{array}\n",
    "    </td>\n",
    "<tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The raw data\n",
    "- e.g., the sequence of words $\\mathbf{s} = \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(\\bar T)}$\n",
    "\n",
    "is not naturally labeled.\n",
    "\n",
    "We need a Data Preparation step to create labeled example $i$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\x^\\ip & = & \\mathbf{s}_{(1)}, \\ldots \\mathbf{s}_{(i)} \\\\\n",
    "\\y^\\ip & = & \\mathbf{s}_{(i+1)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We have called this method of turning unlabeled data into labeled examples: *Semi-Supervised* Learning.\n",
    "\n",
    "In the NLP literature, it is called *Unsupervised Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are abundant sources of raw text data\n",
    "- news, books, blogs, Wikipedia\n",
    "- not all of the same quality\n",
    "\n",
    "The large number of examples that can be generated facilitates the training of models with very large number of weights.\n",
    "\n",
    "This is extremely expensive but, fortunately, the results can be re-used.\n",
    "- Someone with abundant resources trains a Language Model on a broad domain\n",
    "- Publishes the architecture and weights\n",
    "- Others re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next ? Really: predict the *distribution* of next\n",
    "\n",
    "We have casually defined the Language Modeling objective as predicting the next token.\n",
    "\n",
    "As you can see: the head layer is a Classifier\n",
    "- produces a probability for *each token* in the vocabulary as being the next\n",
    "- We choose one token by sampling from this probability distribution\n",
    "    - Greedy sampling: always chose the token with highest probability\n",
    "    - Non-greedy sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Masked Language Modeling objective\n",
    "\n",
    "There is a variation on the Language Modeling objective called the  *Masked Language Modeling* objective.\n",
    "- Language Modeling objective: given $s[1:\\tt-1]$, predict $s[\\tt]$\n",
    "- Masked Language Modeling objective\n",
    "    - Given $s[1:\\tt]$\n",
    "    - Randomly chose an index $1 \\le j \\le \\tt$\n",
    "    - \"Mask\" token $j$ by replacing it with `<MASK>` so that the input becomes\n",
    "    $$\n",
    "    s_{(0)},  \\ldots s_{(j-1)}, \\text{<MASK>}, s_{(j+1)}, \\ldots s_\\tp\n",
    "    $$\n",
    "    - Predict the value behing the mask, e.g., $s_{(j)}$\n",
    "- The Language Modeling objective is the special case where $j=\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Pre-Training + Supervised Fine-Tuning (Transfer Learning)\n",
    "\n",
    "How do we adapt a Language Model to solve other Target tasks ?\n",
    "\n",
    "The obvious answer is via Transfer Learning\n",
    "- The Language Model has learned a lot about the nature of language\n",
    "    - perhaps the language-knowledge can be transfered to a new task\n",
    "- Replace the \"head\" that predicts the next token\n",
    "- With a new task-specific head\n",
    "- Train the new model on labeled examples from the Target task\n",
    "    - the task-specific head **must** be trained\n",
    "    - the language-model weights **can** (but don't have to) be adapted\n",
    "    \n",
    "This paradigm is called *Unsupervised Pre-Traininng + Supervised Fine-Tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transfer_Learning_2.jpg\" width=60%></td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Fine-Tuning a Pre-trained Language Model to analyze sentiment\n",
    "\n",
    "This is a straight-forward application of Transfer Learning\n",
    "- Replace the Classification Head used for Language Modeling\n",
    "    - e.g., a head that generated a probability distribution over words in the vocabulary\n",
    "- By an untrained Binary Classification head (Positive/Negative sentiment)\n",
    "- Train on examples. Pairs of\n",
    "    - sentence\n",
    "    - label: Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other uses of a Language Model: Feature based Transfer Learning\n",
    "\n",
    "We can generalize the procedure of \"replacing the head\": re-use the features produced by the\n",
    "Source model.\n",
    "\n",
    "Let $f_\\Theta (\\x)$ denote the function computed by the Source Language Model on input sequence $\\x$\n",
    "- the output of the layer before the final Classification layer that translates $f_\\Theta (\\x)$ into the token Vocabulary\n",
    "- the Source model is parameterized by $\\Theta$\n",
    "\n",
    "Feature based Transfer Learning computes\n",
    "$$\n",
    "g_\\Phi( f_\\Theta (\\x) )\n",
    "$$\n",
    "for the function $g$ (parameterized by $\\Phi$) computed by a new NN.\n",
    "\n",
    "The case where $g_\\Phi$ is a Classifier is the special case of creating a new Target task specific head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the  representations produced\n",
    "by an Encoder Transformer trained on a Language Modeling task\n",
    "- embody great semantic meaning: meaning of a word in the context of its predecessor words\n",
    "- are much shorter than One Hot Encoding of tokens\n",
    "- are more meaningful than simple Word Embeddings\n",
    "    - embody great semantical meaning\n",
    "\n",
    "We may transfer the \"features\" that these representations encode for the purpose\n",
    "of creating *Context Sensitive Representations* of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the meaning of the word \"bank\" varies with context\n",
    "- a financial institution\n",
    "- the land adjacent to a river\n",
    "- tilting the wings of an airplane\n",
    "\n",
    "Word Embeddings are a unique representation of a word, independent of context, hence cannot\n",
    "fully capture the multiple meanings of the word \"bank\"\n",
    "\n",
    "Using a bi-directional Encoder, the representation becomes a *Context Sensitive Representation*\n",
    "- able to distinguish the multiple meanings of the word \"bank\"\n",
    "- from the context (preceding and succeeding words) in which it appears\n",
    "\n",
    "See the [ELMo paper](https://arxiv.org/abs/1802.05365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An interesting special case\n",
    "- the representation of the first or final token in the sequence\n",
    "    - usually a special token `<START>, <END>`\n",
    "- is a summary of the *entire sequence* (using a bi-directional RNN or non-masking Encoder Transforer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This representation is a very short encoding of a long sequence\n",
    "- like hashing\n",
    "\n",
    "One can use this to implement *Semantic Search*\n",
    "- \"hash\" each page on the Web\n",
    "- \"hash\" a search query\n",
    "- The Search returns those web pages whose hash is similar to the hash of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-task learning\n",
    "\n",
    "One area of recent interesting is *multi-task learning*\n",
    "- Training a model to implement multiple tasks\n",
    "\n",
    "A model that implements a single task computes\n",
    "$$\\pr{\\text{output | input}}$$\n",
    "\n",
    "A model that implements several tasks computes\n",
    "$$\\pr{\\text{output | input, task-id }} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the Universal API, \n",
    "a Language Model may be adapted to solve *multiple tasks* simultaneously.\n",
    "\n",
    "This requires you to construct a training set\n",
    "- with examples from each task\n",
    "- where each example has an additional \"feature\" that identifies the task to which it applies\n",
    "\n",
    "For example\n",
    "\n",
    "$$\\begin{array}[lll] \\\\\n",
    "(  \\mathsf{Translate \\; to \\;French} , & \\text{English text} ,  & & \\text{French Text}) \\\\\n",
    "( \\mathsf{Answer \\; the \\; question} , & \\text{document} , & \\text{question} , & \\text{answer}) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The first feature above is the task identifier of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
