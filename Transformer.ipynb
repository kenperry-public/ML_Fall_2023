{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with Sequences: Recurrent Neural Network (RNN) layer\n",
    "\n",
    "For a function that takes \n",
    "sequence $\\x^\\ip$ as input\n",
    "and creates sequence $\\y$ as  output we had two choices for implementing the function.\n",
    "\n",
    "The RNN implements the function as a \"loop\"\n",
    "- A function that taking **a single** $\\x_\\tp$ as input a time\n",
    "- Outputting $\\y_\\tp$ \n",
    "- Using a \"latent state\" $\\h_\\tp$  to summarize the prefix $\\x_{(1\\ldots \\tt)}$\n",
    "- Repeat in a loop over $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\h_\\tp | \\x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } [ \\x_{(1)} \\dots \\x_\\tp ]\\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Loop with latent state</strong></center>\n",
    "    <img src=\"images/RNN_arch_loop.png\" width=70%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unrolling\" the loop makes it equivalent to a multi-layer network\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_many_to_many.jpg\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer (Encoder style)\n",
    "\n",
    "The alternative to the loop was to create a \"direct function\"\n",
    "- Taking a **sequence** $\\x_{(1 \\dots \\tt)}$ as input\n",
    "- Outputting $\\y_\\tp$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function</strong></center>\n",
    "    <img src=\"images/RNN_arch_parallel.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to output the sequence $\\y_{(1)} \\ldots \\y_{(T)}$ we\n",
    "create $T$ copies of the function (one for each $\\y_\\tp$)\n",
    "- computes each $\\y_\\tp$ in **parallel**, not sequentially as in the loop\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel_masked.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The parallel units constitute a *Transformer Encoder*\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Encoder (causal masked input)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_1.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compared to the unrolled RNN, the Transformer Decoder\n",
    "- Takes a **sequence** $\\x_{(1..t)}$ as input\n",
    "    - Because $\\y_\\tp$ is computed as a *direct* function of the prefix $\\x_{(1..t)}$ rather than recursively\n",
    "- Has **no** latent state: output is a direct function of the input sequence\n",
    "- Has **no** data (e.g., $\\h_\\tp)$ passing from the computation between time steps (e.g., from $\\tt$ to $(\\tt +1)$)\n",
    "- Outputs generated in parallel, not sequentially\n",
    "- No gradients flowing backward over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this architecture, we can compute more general functions than the RNN\n",
    "- where each $\\y_\\tp$ depends on the entire $\\x_{(1 \\ldots T)}$ rather than a prefix $\\x_{(1 \\ldots \\tt)}$\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Direct function, in parallel (un-masked input)</strong></center>\n",
    "<img src=\"images/Transformer_parallel.png\" width=50%>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Decoder (unmasked input)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_2.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example of such a general function is the \"meaning\" of a word, in context.\n",
    "\n",
    "| Sentence | Meaning of \"it\" |\n",
    "|:----------|:-----------------|\n",
    "The animal didn't cross the street because **it** was too tired | the animal\n",
    "The animal didn't cross the street because **it** was too wide  | the road\n",
    "\n",
    "The meaning of the word \"it\" is determined by a word that follows it (\"tired\" or \"wide\")\n",
    "\n",
    "So even though the Transformer output at each position is a function of the entire sequence $\\x_{(1 \\ldots T)}$\n",
    "- the output is different for each position $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can control whether the input to Transformer element at position $\\tt$ is\n",
    "prefix $\\x_{(1 \\ldots \\tt)}$\n",
    "or\n",
    "the entire sequence $\\x_{(1 \\ldots T)}$\n",
    "by **masking** the input to element $\\tt$\n",
    "- no masking: the entire sequence is visible\n",
    "- *casual masking*: only the prefix up to $\\tt$ is visible: $\\x_{(1 \\ldots \\tt)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention is all you need\n",
    "\n",
    "The key to the Transformer is multiple uses of the Attention mechanism\n",
    "- Self-attention\n",
    "    - Both the Encoder and Decoder attend to *their own inputs*\n",
    "    - Implementing the *direct function*, rather than \"loop\" approach\n",
    "- Cross-attention\n",
    "    - The Decoder attends to the intermediate representations of the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The queries used in attention lookup are $\\h_\\tp, \\bar \\h_\\tp$\n",
    "- as are the keys and values\n",
    "\n",
    "We will use $d$ (or $d_\\text{model}$ to denote the length of these values.\n",
    "\n",
    "Moreover, all pathways in the Transformer will maintain the size $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Technical clarifications\n",
    "\n",
    "## Shared Transformer blocks across positions\n",
    "\n",
    "The transformer blocks (\"circles\" in the diagram)\n",
    "- are **shared** across all positions\n",
    "- that is: the same computation (with shared parameters) is performed in parallel\n",
    "- Thus, the number of parameters is **not** a function of sequence length $T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Identifying $\\hat\\y_\\tp$ with $\\h_\\tp$\n",
    "\n",
    "The simplest RNN (corresponding to our diagrams) use the latent state $\\h_\\tp$ as the output $\\hat\\y_\\tp$\n",
    "$$\n",
    "\\hat\\y_\\tp = \\h_\\tp\n",
    "$$\n",
    "\n",
    "It is easy to add another NN to transform $\\h_\\tp$ into a $\\hat\\y_\\tp$ that is different.\n",
    "\n",
    "- We can add a NN to the Decoder RNN that implements a function $D$ that transforms the latent state into an output.\n",
    "\n",
    "$$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "Here is what the additional NN looks like:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_no_attention.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of the Transformer:we will assume the style of a single output $\\h_\\tp$\n",
    "\n",
    "The reason for doing this:\n",
    "- We can \"stack\" $N$ Transformer layers (just as we can stack RNN layers)\n",
    "- The output of the non-top layer $j$ is $\\h^{[j]}_\\tp$, not the final $\\y_\\tp$\n",
    "- We identify $\\y_\\tp$ as the output of the top layer $\\h^{[N]}_\\tp$\n",
    "    - perhaps after a further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Furthermore: \n",
    "    \n",
    "Since the Encoder part is no longer a \"loop\"\n",
    "- It is inaccurate to refer to the Encoder output $\\bar \\h_\\tp$ as a \"latent\" state\n",
    "- However, $\\bar \\h_\\tp$ *is still* a summary of the input sequence\n",
    "    - a summary of $\\x_{(1 \\ldots \\tt)}$ when casual attention is used\n",
    "    - a summary of $\\x_{(1 \\ldots \\bar T)}$ otherwise\n",
    "- Out of **bad habit** we may continue to erroneously refer to $\\bar \\h$ and $\\h$ as \"latent\" states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Transformer Encoder: Self Attention\n",
    "\n",
    "There are two \"styles\" of Transformer\n",
    "- Encoder\n",
    "- Decoder\n",
    "\n",
    "They are often paired into an Encoder-Decoder architecture but may also be used stand-alone.\n",
    "\n",
    "We begin our discussion of the Transformer by looking at the Encoder style Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we look inside the box computing the direct function, we will find several layers\n",
    "- An Attention layer\n",
    "    - To influence which elements of the input sequence $\\x$ to attend/focus when outputting $\\y_\\tp$\n",
    "- A Feed Forward Network (FF) layer to compute the function\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above diagram shows an Encoder that takes an input sequence $\\x_{(1 \\ldots T)}$\n",
    "- consistent with our diagram of an Encoder computing a direct function of the entire input sequence\n",
    "\n",
    "When the Attention is directed at the Encoder's own inputs: we call this *Self-Attention*.\n",
    "\n",
    "Moreover, the Attention allows access to all elements of the input sequence\n",
    "- allowing the computation of a direct function of all elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Advantages of a Transformer compared to an RNN\n",
    "\n",
    "As we will demonstrate in detail below\n",
    "- The Transformer's operations can be performed in parallel versus sequentially for the RNN\n",
    "    - parallel processing of each element of the output sequence\n",
    "    - number of steps to produce an output sequence of length $T$\n",
    "        - is constant, rather than $T$\n",
    "    - faster !\n",
    "- Gradients less likely to vanish or explode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can **leverage** these advantages in complexity by\n",
    "- By making a Transformer model bigger (e.g., more stacked Transformer layers)\n",
    "- Making the sequence lengths longer\n",
    "- Increasing the number of examples of training data\n",
    "\n",
    "So, for the same time \"cost\" as an RNN, we can use a bigger Transformer on more data\n",
    "- **Hence: we can learn more complex functions for similar time cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **path length** from the output to the input is constant in an Transformer, compared to $T$ in the RNN.\n",
    "- parallel computation: reduced wall time\n",
    "- less likely for gradients to vanish/explode over shorter path\n",
    "    - Transformer better ble to capture long-range dependencies than an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But there are costs to pay for this (relative to an RNN), many due to Attention Lookup.\n",
    "\n",
    "- higher memory \n",
    "    - the $Q$ matrix is shape $(T \\times d)$; the $K, V$ matrices are $(\\bar T \\times d)$\n",
    "        - internal dimensions are size $d$\n",
    "        - One query for each of the $T$ positions of the output sequence\n",
    "            - or, as we will see in the Encoder-Decoder combination\n",
    "                - $\\bar T$ outputs (\"latent states\") of the Encoder to attend to\n",
    "    - intermediate matrices, e.g. \n",
    "    $$Q * K^T$$\n",
    "    are of shape $(T \\times \\bar T)$\n",
    "- the number of operations is greater\n",
    "\n",
    "- the number of parameters is greater\n",
    "\n",
    "We give the detailed math below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of sequential steps\n",
    "\n",
    "The most obvious advantage of the \"direct function\" as opposed to the \"loop\" is\n",
    "that outputs are computed in parallel versus sequentially.\n",
    "\n",
    "For an input sequence of length $T$:\n",
    "- The loop requires $T$ steps\n",
    "- The direct function requires $1$ step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Path length\n",
    "The *Path length* is the distance that the Loss Gradient needs to travel backwards during Back Propagation.\n",
    "\n",
    "At each step, the gradient is subject to being diminished or increased (Vanishing/Exploding gradients).\n",
    "\n",
    "Since the Transformer operates in parallel across positions, this is $\\OrderOf{1}$.\n",
    "\n",
    "It is $\\OrderOf{T}$ for the RNN due to the sequential computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The constant path length is critical to the success of the Transformer**\n",
    "- The query used for the input at position $\\tt$ can access **all** prior positions $\\tt' \\le \\tt$ at the same cost\n",
    "    - Gradient not diminished\n",
    "    - RNN\n",
    "        - Gradient signal diminished for position $\\tt' << \\tt$\n",
    "        - Truncated Back Propagation may kill the gradient flow from position $\\tt$ back to $\\tt'$ beyond truncation window\n",
    "\n",
    "A key strength of the Transformer is that it enables learning long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of parameters\n",
    "\n",
    "In the Transformer,\n",
    "the $Q, K, V$ matrices are first projected through $(d \\times d)$ matrices, $\\W_k, \\W_Q, \\W_V$\n",
    "   \n",
    "out  &nbsp;  &nbsp;  &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:-:|:-:|:-:\n",
    "$Q$ | = | $Q$| * |$\\W_Q$ |\n",
    "$(T \\times d)$ | | $(T \\times d)$ | | $(d \\times d)$\n",
    "&nbsp;\n",
    "$K$ | = | $K$| * |$\\W_K$ |\n",
    "$V$ | = | $V$| * |$\\W_V$ |\n",
    "$(\\bar T \\times d)$ | | $(\\bar T \\times d)$ | | $(d \\times d)$\n",
    "\n",
    "Each of the matrices, $\\W_k, \\W_Q, \\W_V$, is $\\OrderOf{d^2}$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Feed Forward Network is usually implemented by 2 `Dense` layers.\n",
    "\n",
    "the first takes the length $d$ attention output\n",
    "- and creates $d_\\text{ffn}$ new features\n",
    "- by custom: $d_\\text{ffn} = 4 * d$\n",
    "\n",
    "The second takes the length $d_\\text{ffn}$ output of the first and creates the final length $d_\\text{model}$ output.\n",
    "\n",
    "Thus, each `Dense` layer has $\\OrderOf{ d^2 }$ weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Number of operations\n",
    "\n",
    "What about the number of operations ? \n",
    "\n",
    "The Attention Lookup is computed via matrix multiplication \n",
    "    $$\n",
    "    Q * K^T * V\n",
    "    $$\n",
    "    \n",
    "$Q * K^T$ has $(T \\times \\bar T)$ elements, each the result of $d$ multiplications.\n",
    "\n",
    "Thus: $\\OrderOf{T^2 *d}$ multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Self Attention layer attend to (transformed) inputs\n",
    "- each element assumed size of $d$\n",
    "\n",
    "The keys and values of the CSM implementing Attention are the size $d$ input elements.\n",
    "- Each attention lookup (dot product of query with a key) requires $d$ multiplications.\n",
    "- There are $T$ key/value pairs in the CSM\n",
    "- There are $T$ attention units (one for each position, outputting $\\h_\\tp$)\n",
    "\n",
    "Thus: $\\OrderOf{T^2 *d}$ multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNN calculations\n",
    "\n",
    "Let's examine the RNN's number of operations and weights.\n",
    "\n",
    "The RNN inputs $\\x_\\tp$ and outputs $\\h_\\tp$  of size $d$ (same as Transformer).\n",
    "- In the RNN $\\h_\\tp$ is also the latent state\n",
    "\n",
    "Each step of the RNN updates the latent state $\\h_\\tp$ via the equation\n",
    "    $$\n",
    "\\h_\\tp  =  \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \n",
    "$$\n",
    "\n",
    "The  weight matrices \n",
    "$$\\W_{xh} \\text{ and } \\W_{hh}$$\n",
    "are of size $$\\OrderOf{d \\times d}$$\n",
    "- transforming length $d$ vectors ($\\x_\\tp, \\h_{(\\tt-1)}$) into a length $d$ vector $\\h_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The multiplication of $(d \\times d)$ weights matrices times a vector of length $d$\n",
    "- requires $d$ multiplications per element\n",
    "- there are $d$ elements in $\\h_\\tp$\n",
    "\n",
    "Thus $\\OrderOf{d^2}$ operations per time step.\n",
    "\n",
    "There are $T$ *sequential* time-steps\n",
    "- $\\OrderOf{T * d^2}$ total operations\n",
    "- involving  $T$ sequential steps\n",
    "    - steps are computed sequentially in the RNN, versus in parallel in the Transformer\n",
    "- path length $T$ as gradient flows backward through each of the $T$ time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complexity: summary\n",
    "\n",
    "We also throw in a CNN for comparison\n",
    "\n",
    "The detailed CNN math is given in a following section.\n",
    "\n",
    "| Type | Parameters  | Operations  &nbsp; &nbsp; &nbsp; | Sequential steps | Path length\n",
    "|:------|:---|:---|:---|:---|\n",
    "|  CNN | $\\OrderOf{k * d^2}$   | $\\OrderOf{T * k * d^2}$ | $\\OrderOf{T}$   | $\\OrderOf{T}$ |\n",
    "| RNN  | $\\OrderOf{d^2}$       | $\\OrderOf{T * d^2}$     | $\\OrderOf{T}$    | $\\OrderOf{T}$ |\n",
    "| Self-attention | $\\OrderOf{d^2} $ | $\\OrderOf{T^2 *d}$ | $\\OrderOf{1}$ | $\\OrderOf{1}$ |\n",
    "\n",
    "Reference:\n",
    "- [Transformer Scaling paper](https://arxiv.org/pdf/2001.08361.pdf#page=6)\n",
    "- [Table 1 of Attention paper](https://arxiv.org/pdf/1706.03762.pdf#page=6)\n",
    "- See [Stack overflow](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model) for correction of the number Operations calculated in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transformer main point of comparison to the RNN\n",
    "- fewer Sequential Steps: $\\OrderOf{1}$ versus $\\OrderOf{T}$\n",
    "- operations: $\\OrderOf{T^2 * d}$ versus $\\OrderOf{T * d^2}$\n",
    "    - more when sequences are long, i.e., $T \\gt d$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**But:** because of the reduced number of sequential steps, Transformers\n",
    "- can stack *many* (i.e.,  $n_\\text{layers}$) blocks, each taking $\\OrderOf{1}$ time\n",
    "    - $\\OrderOf{n_\\text{layers}}$ Sequential Steps total\n",
    "- and still be less than the $\\OrderOf{T}$ Sequential Steps of an RNN\n",
    "- at the cost of increasing number of operations and parameters by $\\OrderOf{n_\\text{layers}}$\n",
    "\n",
    "Transformers consume larger number of parameters and operations through this factor of $n_\\text{layers}$ blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CNN calculations\n",
    "\n",
    "Here's the details of the math for the CNN\n",
    "\n",
    "- path length $T$ \n",
    "    - each kernel multiplication connects only $k$ elements of $\\x$\n",
    "    - since kernels overlap inputs, can't parallelize, hence $\\OrderOf{T/k}$ path length\n",
    "        - can reduce to $\\log(T)$ with tree structure\n",
    "- Parameters\n",
    "    - kernel size $k$\n",
    "    - number of input channels = number of output channels = $d$\n",
    "    - $k *d$ parameters for kernel of one channel\n",
    "    - $\\OrderOf{k * d^2}$ parameters for kernel for all $d$ output channels\n",
    "    \n",
    "- Operations\n",
    "    - for a single output channel: $k$ per input channel\n",
    "        - There are $d$ input channels, so $k *d$ for each dot product of *one* output channel\n",
    "        - There are $d$ output channels, so $k * d^2$ per time step\n",
    "    - $T$ time steps so $\\OrderOf{T * k * d^2}$ number of operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A free lunch ? Almost !\n",
    "\n",
    "Transformers sound almost too good to be true\n",
    "- Faster compute (through reduced number of Sequential steps)\n",
    "- Constant Path Length\n",
    "    - Better able to capture long range dependencies\n",
    "    \n",
    "Is there really such a thing as a free lunch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Almost.\n",
    "\n",
    "In order to achieve the full benefit of reduced path length\n",
    "- the operations across all $T$ positions must be computed in parallel\n",
    "- this involves a tremendous amount of simultaneous compute power\n",
    "    - very expensive in hardware and power costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, *positional encoding* needs to be preserved at each layer\n",
    "- to maintain relative ordering (e.g., for causal attention)\n",
    "- more complicated than an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: Decoder\n",
    "\n",
    "The second type of Transformer is called the Decoder style.\n",
    "\n",
    "It is often pair with an Encoder to form and Encoder-Decoder architecture but is also use stand-alone.\n",
    "\n",
    "A Decoder usually operates in an *auto regressive* manner\n",
    "- it has **no initial** input\n",
    "- the output $\\hat y_\\tp$ of time step $\\tt$ is appended to the input\n",
    "    - so the input at time step $\\tt$ is $$\\hat\\y_{(1 \\ldots \\tt-1)}$$\n",
    "\n",
    "When this occurs, the Encoder at time step $\\tt$ can only attend to a *prefix* of $\\hat\\y_{(1..T)}$\n",
    "$$\n",
    "\\hat\\y_{(1 \\ldots \\tt-1)}\n",
    "$$\n",
    "\n",
    "This is implemented by a form of Attention called **Masked** Self-Attention\n",
    "-  $\\hat\\y_{(1..T)}$ is masked to make visible only the prefix $\\hat\\y_{(1 \\ldots \\tt-1)}$\n",
    "\n",
    "We will introduce the Decoder style Transformer as part of the Encoder-Decoder architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: (Encoder/Decoder style)\n",
    "\n",
    "It is common to use two Transformers in an Encoder/Decoder configuration.\n",
    "\n",
    "Refer back to our [Attention module](Intro_to_Attention.ipynb#Attention)\n",
    "- example on Question Answering\n",
    "    - a Input consisting of  Context and a Question is processed by the Encoder\n",
    "    - the Output Answer is generated auto-regressively by the Decoder\n",
    "        - attending to the Encoder output\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder part of the pair\n",
    "- has full visibility to the input sequence $\\x_{(1 \\ldots \\bar T)}$\n",
    "    - via Self-Attention\n",
    "- it transformers the input into sequence $\\bar \\h_{(1 \\ldots \\bar T)}$\n",
    "    - which is made available to the Decoder when generated each element of the Output $\\hat \\y_\\tp$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder part of the pair\n",
    "- can attend to the Encoder Input $\\bar \\h_{(1 \\ldots \\bar T)}$ when generating any Output $\\hat \\y_\\tp$\n",
    "- the Output is produced auto-regressively\n",
    "    - so Output at position $\\tt$ is a function of\n",
    "        - $\\hat\\y_{(1..\\tt-1)}$: the Output generated thus far\n",
    "        - the encoded input $\\bar \\h_{(1 \\ldots \\bar T)}$ \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder uses **two types** of Attention\n",
    "- Attention to the Encoder output $\\bar \\h_{(1 \\ldots \\bar T)}$: Encoder/Decoder **Cross Attention**\n",
    "- Self-Attention to the Output generated thus far $\\hat\\y_{(1..\\tt-1)}$\n",
    "    - **Masked Self-Attention**\n",
    "        - Self-Attention: attends to its own input\n",
    "        - Masked: access only to prefix $\\hat\\y_{(1..\\tt-1)}$ rather than full $\\hat\\y_{(1..T)}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Decoder.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The combinded Encoder-Decoder Transformer diagram looks like this\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer Layer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_2.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Explanation of diagram**\n",
    "- The Encoder uses Self-attention (<span style=\"color:green\">wide Green arrow</span>) to attend to input sequence $\\x$\n",
    "- The Decoder uses Masked Self-attention (<span style=\"color:red\">wide Red arrow</span>) to attend to its input\n",
    "    - It's input is the prefix of the output sequence $\\y$\n",
    "    - Limited to prefix of length $\\tt$ by **masking**\n",
    "- The Decoder uses Cross Attention (between Encoder and Decoder) <span style=\"color:blue\">(wide Blue arrow)</span>\n",
    "    - To enable Decoder to focus on which Encoder latent state $\\bar \\h_\\tp$ to atttend to\n",
    "- The dotted <span style=\"color:blue\">(thin Blue arrow)</span> indicates that the output $\\hat \\y_\\tp$ is appended to the input that is available when generating $\\hat \\y_{(\\tt+1)}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the Decoder is recurrent (auto-regressive; generative)\n",
    "- it generates a single output at a time\n",
    "- unlike the Encoder, which generates all outputs (i.e., \"encodings\") in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During training, at step $\\tt$\n",
    "- the entire Target $\\y_{(1..T)}$ is available as input\n",
    "- **but** Causal masking ensures that only $\\y_{(1..\\tt-1)}$ is visible\n",
    "- so the *available* input at step $\\tt$ is $\\y_{(1..\\tt-1)}$\n",
    "- note that Training time input is $\\y_{(1..\\tt-1)}$ **not** $\\hat \\y_{(1..\\tt-1)}$\n",
    "    - Teacher forcing to prevent cascading errors\n",
    "    - stops errors at step $\\tt-1$ from affecting predictions at subsequent steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functional versus Sequential architecture\n",
    "\n",
    "The architecture diagram is more complex than we have seen thus far.\n",
    "\n",
    "In particular: data no longer strictly flows forward in a layer-wise arrangement !\n",
    "- There are two independent sub-networks (Encoder and Decoder)\n",
    "- Connection from the Encoder output to the middle of the Decoder (Cross-Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each of the Encoder and Decoder is an independent Functional model.\n",
    "- not our familiar Sequential modles\n",
    "\n",
    "The Encoder/Decoder pair combination is also constructed as a Functional model.\n",
    "\n",
    "Since we have not yet addressed Functional Models, you may not be prepared to completely grasp the totality.\n",
    "\n",
    "But hopefully you can absorb the concepts even without fully understanding the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Detailed Encoder/Decoder Transformer architecture\n",
    "\n",
    "There are other components of the Encoder and Decoder that we have yet to describe.\n",
    "\n",
    "We will do so briefly.\n",
    "\n",
    "(The Transformer was introduced in the paper [Attention is all you Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "   \n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Embedding layers**\n",
    "\n",
    "We will motivate and describe Embeddings in the NLP module.\n",
    "\n",
    "For now:\n",
    "- an embedding is an encoding of a categorical value that is shorter than OHE\n",
    "\n",
    "It is used in the Transformer to\n",
    "- encode the input sequence of words\n",
    "- encode the output sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Positional Encoding**\n",
    "\n",
    "The inputs are ordered (i.e., sequences) and thus describe a relative ordering relationship between elements.\n",
    "\n",
    "But inputs to most layer types (e.g., Fully Connected) are unordered.\n",
    "\n",
    "The Positional Encoding is a way of encoding the the relative ordering of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To represent the relative position of each element in the sequence,\n",
    "- we can pair the input element  with an encoding of its position in the sequence.\n",
    "$$\n",
    "\\langle \\x_\\tp, \\text{encode}(\\tt) \\rangle\n",
    "$$\n",
    "\n",
    "The box labeled \"Positional Encoding\" creates $\\text{encode}(\\tt)$.\n",
    "\n",
    "The \"+\" concatenates the Input Embedding and Positional Encoding to create $\n",
    "\\langle \\x_\\tp, \\text{encode}(\\tt) \\rangle\n",
    "$.\n",
    "\n",
    "If relative position is important, the NN can learn the values of  $\\text{encode}(\\tt)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The encoding is subtle.\n",
    "\n",
    "A fuller explanation is given in this [module](Transformer_PositionalEmbedding.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Self Attention layers (Encoder and Decoder)**\n",
    "\n",
    "The 3 arrows flowing into the Multi-Head Attention box\n",
    "- are identical\n",
    "- are the inputs (after being Embedded and having Positional Encoding added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Self-Attention layers for the Encoder and Decoder\n",
    "- **differ in that the Decoder uses Causal Masking versus no-masking for the Encoder**\n",
    "- Decoder can't \"look ahead\" at output $\\y_{\\tt'}$ for $\\tt' \\ge \\tt$\n",
    "    - it hasn't been generated yet at test time step $\\tt$\n",
    "    - it **is** available at training time (via Teacher Forcing)\n",
    "        - but shouldn't look at it during training time, in order for training to be similar to test time\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cross Attention layer (Decoder)**\n",
    "\n",
    "The two arrows flowing from the Encoder output are the keys and values of the CSM\n",
    "\n",
    "The arrow flowing from the Self Attention layer is the query\n",
    "- The output of the Self Attention layer is the **query** used in Cross Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Add and Norm**\n",
    "\n",
    "We have seen each of these layer types before\n",
    "- Norm: Batch (or other) Normalization layers\n",
    "- Add: the part of the residual network that joins outputs of multiple previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The diagram shows an Encoder/Decoder pair.\n",
    "\n",
    "You will notice that each element of the pair is different.\n",
    "\n",
    "- It is possible to use each element independently as well.\n",
    "\n",
    "- But first we need to understand\n",
    "the source of the differences and their implications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How is the direct function computed ?\n",
    "\n",
    "The Encoder uses self-attention\n",
    "- So the keys and values of the CSM are derived directly from input sequence $\\x_{(1 \\ldots T)}$\n",
    "\n",
    "During training, the Encoder\n",
    "- learns a query, derived from input sequence $\\x_{(1 \\ldots T)}$\n",
    "- learns weights for the Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Attention output \n",
    "- is equal to a weighted combination of CSM values\n",
    "    - i.e., weighted sum of input elements\n",
    "\n",
    "The Feed Forward Network transforms the Attention output into Encoder output $\\bar \\h_\\tp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly for the Decoder.\n",
    "\n",
    "The Self-Attention layer CSM  has keys and values that are incrementally constructed\n",
    "from the outputs $\\y_{(1 \\ldots, \\tt)}$ that have been created from the first $\\tt$ steps.\n",
    "\n",
    "The Cross-Attention layer CSM has keys and values that are outputs $\\bar \\h_\\tp$ of the Encoder.\n",
    "\n",
    "During training, the Self-Attention layer outputs **the query** that is used for Cross Attention.\n",
    "\n",
    "The query is created by self-attention to the inputs.\n",
    "\n",
    "The Decoder **learns (from training)** \n",
    "- the Self-Attention query \n",
    "- the Cross Attention query\n",
    "- the weights of the Feed Forward Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacked Transformer\n",
    "\n",
    "Just as with many other layer types (e.g., RNN), we may stack Transformer layers.\n",
    "- Each layer creating alternate representations of the input of increasing complexity\n",
    "\n",
    "In fact, stacking $N > 1$  Transformer layers is typical.\n",
    "\n",
    "$N = 6$ was the choice of the original paper.\n",
    "\n",
    "Note that this is still an Encoder/Decoder\n",
    "- so the *final* output of the Encoder is attended to by *each* layer of the Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Stacked Transformer Layers (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_multi.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use cases for each style of Transformer\n",
    "\n",
    "The Transformer for the Encoder and Decoder of an Encoder/Decoder Transformer are slightly different.\n",
    "\n",
    "They can also be used individually as well as in pairs.\n",
    "\n",
    "It's important to understand the differences in order to know when to use each individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoder/Decoder uses\n",
    "\n",
    "The Encoder/Decoder acts as a function\n",
    "- from Input, processed by the Encoder\n",
    "- to Target, processed by the Decoder\n",
    " \n",
    "This is a natural architecture for Sequence to Sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoder only uses\n",
    "\n",
    "The Encoder side of the pair **does not** restrict the order in which it's inputs are accessed.\n",
    "- Self-attention **without** causal masking\n",
    "\n",
    "Thus the Encoder output at *each* position is a function of the input at *all* positions.\n",
    "\n",
    "This is valuable for tasks that require a context-sensitive representation of\n",
    "each input element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example: the meaning of the word \"**it**\" changes with a small change to a subsequent word in the following sentences:\n",
    "- \"The animal didn't cross the road because **it** was too tired\"\n",
    "\n",
    "- \"The animal didn't cross the road because **it** was too wide\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some tasks with this characteristic are\n",
    "- Sentiment\n",
    "- Masked Language Modeling: fill-in the masked word\n",
    "- Semantic Search\n",
    "    - compare a summary of the sequence that is the context-sensitive representation of\n",
    "        - query sentence\n",
    "        - document sentences\n",
    "    - Each summary is a kind of **sentence embedding**\n",
    "    - Summary\n",
    "        - pooling over each word\n",
    "        - final token\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is often the case that special tokens are added to the input of an Encoder style transformer.\n",
    "- Designating a special role for this token, compared to the other tokens in the sequence\n",
    "- For example `<CLS>` (\"Classification\") is the single token used as input to a subsequent Classifier layer\n",
    "\n",
    "Thus Encoder style Transformers are usually used as the first \"layer\" of a multi-layer network\n",
    "- later layers being, e.g., task-specific \"heads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder only uses\n",
    "\n",
    "A Decoder style Transformer\n",
    "- looks like the Decoder side of the Encoder-Decoder\n",
    "- *without* Cross-Attention, since there is no Encoder\n",
    "\n",
    "One notable aspect of the Decoder is its auto-regressive behavior\n",
    "- Initial input is empty\n",
    "- Output $\\hat\\y_{(\\tt-1)}$ is appended to the Decoder inputs available at step $\\tt$.\n",
    "- step $\\tt$ input: $\\hat\\y_{([0:\\tt-1])}$\n",
    "\n",
    "Thus, a Decoder only Transformer is useful for completely *generative* task\n",
    "- create sequence output\n",
    "- from no input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can modify a Decoder only Transformer to implement a function from Input to Target\n",
    "- just like an Encoder/Decoder\n",
    "- by initializing the Decoder input to the function input sequence $\\x_{(0..\\bar T)}$\n",
    "\n",
    "Thus, a Decoder only Transformer become similar in function to an Encoder/Decoder Transformer.\n",
    "- but with half the parameters (since no Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The Transformer architecture has come to dominate tasks with long sequences (e.g., NLP).\n",
    "\n",
    "The operations of a Transformer occur in parallel for each position.\n",
    "\n",
    "This allows us to leverage the compute time\n",
    "- Use many stacked Transformer layers\n",
    "- At time cost still less than a sequential RNN layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, the constant path length means the gradients are less likely to vanish/explode for long sequences\n",
    "- No need to truncate Back Propagation as in an RNN\n",
    "- Long term dependencies between positions become feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We pay for these advantages in terms of increasing\n",
    "- number of operations\n",
    "    - but they occur in parallel, so no increase in elapsed time\n",
    "- number of weights\n",
    "\n",
    "Thus, Transformer training is both compute and memory intensive.\n",
    "- This limits the number of individuals/organizations able to train very large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
