{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bigger = Better ?  Scaling laws\n",
    "\n",
    "There are many LLM's, with varying choices of\n",
    "- number of parameters $N$\n",
    "-size of training data $D$\n",
    "    - number of tokens trained on\n",
    "    - not *distinct* tokens in dataset\n",
    "        - same token encountered in each epoch is counted once per epoch\n",
    "- amount of compute for training $C$\n",
    "\n",
    "Here is a table from the [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf#page=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/compute_by_model.png\" width=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have already seen that some LLM properties\n",
    "- like in-context learning (zero or few shot)\n",
    "- \"emerge\" only when model size passes a threshold\n",
    "\n",
    "This argues for bigger models.\n",
    "\n",
    "<img src=\"images/LM_Few_Shot_Accuracy.png\" width=80%>\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is also evidence that the emergence of ability to perform some in-context tasks\n",
    "- is sudden\n",
    "- rather than gradual\n",
    "as the number of parameters increase.\n",
    "\n",
    "<img src=\"images/arithmetic_LLM_by_size.png\">\n",
    "\n",
    "Attribution: [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf#page=46)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is bigger $N$ always better ?\n",
    "\n",
    "Consider the costs.  Larger $N$\n",
    "- entails more computation: larger $C$\n",
    "- probably requires more training data: larger $D$\n",
    "\n",
    "If we fix a \"budget\" for one choice (e.g., $C$) we can explore choices for $N, D$ that meet this budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are two models with the same $C$ budget\n",
    "- but vastly different $N$ and $D$\n",
    "\n",
    "\n",
    "model | Compute (PF-days) | params (M) | training tokens (B) |\n",
    ":---|:---|:---|:---\n",
    "RoBERTa-Large | 49.3 | 355 | 2000\n",
    "GPT-3 2.7B    | 55.2 | 2650 | 300\n",
    "\n",
    "Attribution: [GPT-3 paper](https://arxiv.org/pdf/2005.14165.pdf#page=46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given these choices: how do we choose ?\n",
    "\n",
    "One way to quantify the decision is by setting a goal\n",
    "- to maximize \"performance\"\n",
    "- where this is usually proxied by \"minimizing test loss\" $L$\n",
    "    - Cross Entropy for the \"predict the next\" token task of the LLM\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can state some basic theories\n",
    "- Increasing $N$ creates the *potential* for better performance $L$\n",
    "- To *actualize* the potential\n",
    "    - we need increased $C$\n",
    "        - more parameters via increasing the number of stacked Transformer Blocks\n",
    "    - we need increased $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But this still leaves many unanswered questions\n",
    "- Can $L$ always be reduced ?\n",
    "    - Does performance hit a \"ceiling\" \n",
    "    - For a fixed $N$: perhaps increasing $D$ or $C$ won't help\n",
    "- What is the relationship between $N$ and $D$ ?\n",
    "    - how much must $D$ by increased when $N$ increases\n",
    "- For a fixed $D$: what is the best choice for $N$ ?\n",
    "    - holding performance constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling Laws: early research\n",
    "\n",
    "Fortunately, this\n",
    "[paper](https://arxiv.org/pdf/2001.08361.pdf)\n",
    "has \n",
    "- conducted an empirical study of models with varying $N, D, C$ and resulting $L$\n",
    "- fit an empirical function (*Scaling Laws*) describing the dependency of $L$ on $N, D, C$.\n",
    "\n",
    "We briefly summarize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Performance\" (test loss $L$ ) depends on scale.\n",
    "\n",
    "Scale consists of 3 components\n",
    "- Compute $C$\n",
    "- Dataset size $D$\n",
    "- Parameters $N$\n",
    "\n",
    "We can set a \"budget\" for any of variables $L, N, D, C$\n",
    "- and examine trade-offs for the non-fixed variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper shows that \n",
    "- Increasing your budget for one of the scale factors\n",
    "- increases performance (decrease loss)\n",
    "- **provided** the other two factors don't become bottlenecks\n",
    "\n",
    "<img src=\"images/scaling_loss_v_scale.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But bottlenecks are a worry:\n",
    "- The potential performance of a model of fixed size $N$ hits a \"ceiling\"\n",
    "- That can't be overcome by increasing compute $C$\n",
    "\n",
    "<img src=\"images/scaling_loss_vs_compute.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Observation**\n",
    "\n",
    "For a fixed Compute $C$\n",
    "- a smaller model (that has reached its asymptotic minimum) has lower loss\n",
    "- provided that there is enough training data\n",
    "\n",
    "For a fixed $L$\n",
    "- a smaller model reaches the loss with less compute\n",
    "\n",
    "This is interesting in that more data $D$ may compensate for fewer parameters\n",
    "- we may be able to create \"small\" models (fewer parameters)\n",
    "- with performance equal to larger models\n",
    "- given sufficient $D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also set a performance budget $L$\n",
    "- and examine the amount of training data $D$ to reach this budget\n",
    "- as $N$ varies\n",
    "\n",
    "<img src=\"images/scaling_loss_vs_datasize.png\">\n",
    "\n",
    "**Observation**\n",
    "\n",
    "For a fixed $D$ \n",
    "- bigger models are more data efficient\n",
    "    - for a given level of loss $L$, a larger model achieves $L$ with fewer tokens\n",
    "- but at a higher $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is one graph that combines $N$ and $D$\n",
    "\n",
    "<img src=\"images/scaling_loss_vs_D_and_N.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key result: how must $D$ scale with $N$ ?**\n",
    "\n",
    "The authors show that Performance (test loss as a function of $N, D: L(N,D)$)\n",
    "improves\n",
    "- as long as $N$ and $D$ are scaled together\n",
    "- optimal relationship\n",
    "\n",
    "$$\n",
    "\\frac{N^{0.74}}{D} = \\text{constant}\n",
    "$$ \n",
    "   - Thus, if $N$ increases by a factor of $8$, $D$ should increase by a factor of $8^{0.74} \\approx 5$\n",
    "   \n",
    "- Performance flattens if one of $N, D$ is fixed while the other increases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [Scaling Laws](https://arxiv.org/pdf/2001.08361.pdf#page=4)\n",
    "show that Loss follows a Power Law as a function of $N, C, D$.\n",
    "\n",
    "[Here](https://arxiv.org/pdf/2001.08361.pdf#page=20) is a summary of the Scaling Laws.\n",
    "\n",
    "<img src=\"images/scaling_power_laws_summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling laws: newer research\n",
    "\n",
    "[Continuing research](https://arxiv.org/pdf/2203.15556.pdf) in the area of scaling\n",
    "- confirms the need to scale $N$ and $D$ together\n",
    "- but with a different scaling relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key result: how must $D$ scale with $N$ ?**\n",
    "\n",
    "$$\n",
    "\\frac{N}{D} = \\text{constant}\n",
    "$$\n",
    "\n",
    "Contrast this result with the original paper's relationship of the constant ratio as\n",
    "\n",
    "$$\n",
    "\\frac{N^{0.74}}{D} = \\text{constant}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A key difference between the two papers is the *learning rate schedule*.\n",
    "\n",
    "Recall: a learning rate moderates the rate a which gradient updates affect the model's weights during Gradient Descent\n",
    "\n",
    "$$\n",
    "\\W_{(\\tt+1)} = \\W_\\tp + \\alpha_\\tt * \\frac{\\partial \\loss_\\tp}{\\partial \\W}\n",
    "$$\n",
    "\n",
    "where $\\alpha_\\tp$ is the rate used at epoch $\\tt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the original paper, the learning rate schedule is *fixed* (constant across epochs)\n",
    "$$\n",
    "\\alpha_\\tp = c\n",
    "$$\n",
    "\n",
    "The newer paper shows that a fixed learning rate *over-estimates* $L(N,D)$ when $D \\lt 130 B$\n",
    "- leading to mis-fitting the empirical relationship\n",
    "\n",
    "This can be avoided via a *variable learning rate* that decays $\\alpha_\\tp$ \n",
    "- to a fixed fraction of the initial rate $\\alpha_{(0)}$\n",
    "- as epoch number $\\tt$ increases\n",
    "\n",
    "Hence the optimal relationship changes from \n",
    "$\\frac{N}{D} = \\text{constant}$ to \n",
    "$\\frac{N^{0.74}}{D} = \\text{constant}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "As in the original paper, Test Loss is fit using empirical data as a function $L(N,D)$ of $N$ and $D$.\n",
    "- but subject to a fixed compute budget $C$\n",
    "- $L(N,D)$ is the early-stopped loss\n",
    "    - not trained to optimal converged $L$\n",
    "    - which would require more than the compute budget $C$\n",
    "    \n",
    "Given this function, one can find optimal $N$ and $D$ for a fixed compute budget $C$\n",
    " \n",
    "$$\n",
    "N_\\text{opt}, D_\\text{opt} = \\argmin{ N, D \\text{ s.t. } C=\\text{FLOPS}(N,D)}{L(N,D)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a very interesting result.\n",
    "- For someone on a fixed compute budget $C$\n",
    "- One can find optimal values for model and data size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The future of large models as seen through Scaling laws\n",
    "\n",
    "The Scaling laws suggest\n",
    "- given a fixed compute budget $C$\n",
    "- there is an optimal number of parameters $N$ and number of training tokens $D$\n",
    "\n",
    "Let's evaluate GPT-3, which pre-existed the scaling laws, in terms of what we now know.\n",
    "- $N_\\text{GPT} = 175B$\n",
    "- $D_\\text{GPT} = 0.3 T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the Scaling Laws:\n",
    "- GPT-3 is *under-trained* in time by a large factor\n",
    "    $$\\frac{C^*(N_\\text{GPT},D_\\text{GPT})}{C_\\text{GPT}} = \\frac{4.4 * 10^{24} \\text{ Flops}}{3.1 * 10^{23}\\text{Flops}} > 10$$\n",
    "- GPT-3 is *under-trained* in number of tokens by a large factor\n",
    "\n",
    "$$\n",
    "\\frac{D^*(N_\\text{GPT})}{D_\\text{GPT}}  = \\frac{4.2 TB}{0.3 TB} \\gt 10\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to take advantage of the 175 billion parameters, GPT-3 needed to be trained\n",
    "- for much longer\n",
    "- on many more tokens\n",
    "\n",
    "Here is a plot of the optimal line\n",
    "- we can see GPT-3 is above the line\n",
    "- too large a $N$/too small a $D$\n",
    "    - for the given $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Chinchilla Optimal Training</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <img src=\"https://assets-global.website-files.com/621e749a546b7592125f38ed/62557f7626b9e103db549c7b_tokens_vs_flops%20(1).png\" width=70%>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        Attribution: https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One implication of these results is\n",
    "- it may not be practical (in terms of compute budget) to *optimally* train models with $N \\gt N_\\text{GPT}$\n",
    "- A 10 trillion parameter model needs 100 times the compute used for GPT-3\n",
    "\n",
    "Given that reality, a likely future world is one of\n",
    "- smaller $N$\n",
    "- trained to optimality\n",
    "- resulting in *better* performance $L$ than a larger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors validated this hypothesis by comparing two models\n",
    "- started with a large model called Gopher with $N_\\text{Gopher} = 280B$\n",
    "- trained a smaller model called Chinchilla with $N_\\text{Chinchilla} = 70 B$\n",
    "- using the same compute $C_\\text{Chinchilla} = C_\\text{Gopher}$\n",
    "- but optimal $D$: $D_\\text{Chinchilla} = 1.4 T$\n",
    "\n",
    "Chinchilla, although only $25\\%$ as large as Gopher\n",
    "- outperforms on many benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Scaling Laws are driving the design of new models.\n",
    "\n",
    "- There are \"clones\" of GPT-3 with similar (or better) performance and many fewer parameters\n",
    "    - at a greatly reduced compute budget.\n",
    "    - [LLaMA](https://arxiv.org/pdf/2302.13971v1.pdf): 13B parameters\n",
    "        - From Meta.  Model weights *are not* freely available\n",
    "    - [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)\n",
    "        - family of models from the [BigScience Workshop](https://bigscience.huggingface.co/); Open-source \n",
    "- The successor ([PaLM 2](https://ai.google/static/documents/palm2techreport.pdf)) to Google's\n",
    "$540B$ parameter PaLM model \n",
    "    - has only $16B$ parameters (in its largest configuration) but performs at a similar levelmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another trend (unrelated to scaling) is to incorporate *non-parametric* knowledge into models\n",
    "- e.g., the Web as a source of \"world\" knowledge\n",
    "\n",
    "With an external knowledge source, a model's parameters\n",
    "- can be fewer\n",
    "- encode \"procedural\" knowledge rather than factual knowledge\n",
    "    \n",
    "Thus, the trend towards models of ever increasing size is probably over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference budget\n",
    "\n",
    "We have been focused on the *training budget*\n",
    "- cost of a forward pass\n",
    "- cost of a backward pass\n",
    "- summed over many training examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But larger models also require more compute (and greater latency) at *inference* time\n",
    "- typical way of increasing model size is stacking more Transformer blocks\n",
    "- deeper stack\n",
    "    - greater latency\n",
    "    - increased compute\n",
    "\n",
    "Since we train a model once\n",
    "- but perform inference many times\n",
    "- perhaps we should focus on the *inference budget* rather than the *training budget*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A detailed examination of [inference cost](https://kipp.ly/transformer-inference-arithmetic/#flops-counting)\n",
    " approximates the number of Flops $F$ for once inference at\n",
    " $$\n",
    " F = n_\\text{layers} * 24 * d_\\text{model}^2\n",
    " $$\n",
    " \n",
    " Since $N$ is proportional to $n_\\text{layers}$\n",
    " - inference cost is proportional to model size $N$\n",
    " \n",
    "So a smaller model size $N$ can reduce our inference budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall\n",
    "- a larger model achieves a given loss level *on fewer tokens* then a smaller model\n",
    "\n",
    "But, fixing the loss level\n",
    "- a smaller model can achieve the same loss\n",
    "- at the cost of training on more tokens\n",
    "\n",
    "The smaller model's inference cost is lower\n",
    "- so trading off more tokens for fewer parameters\n",
    "- may be *inference time* optimal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This idea was explored in ([LlaMa](https://arxiv.org/pdf/2302.13971.pdf))\n",
    "- optimizes inference\n",
    "- by training smaller models\n",
    "- on *more data* than required for a larger model to achieve the same loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>LlAMA loss vs training tokens</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <img src=\"images/llama-training-curves.png\" width=50%>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        Attribution: https://arxiv.org/pdf/2302.13971.pdf#page=3\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the above graph: consider a Loss Level of 1.7\n",
    "- the $N = $ 65B model requires $D = $ 600B tokens\n",
    "- the $N = $ 33B model requires $D = $ 800B tokens\n",
    "\n",
    "The author suggests that, for the same loss level, we can trade off\n",
    "- doubling $N$\n",
    "- for an increase of $D$ by $40 \\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The result is that the current trend in LLM's is to\n",
    "- small models\n",
    "- trained on many tokens\n",
    "- so as to optimize the inference budget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "The Scaling laws demonstrate that\n",
    "- Larger models have the *potential* for smaller loss than smaller models\n",
    "    - but require more  (compared to a smaller model) data and compute  to achieve their potential\n",
    "- For a fixed Loss level, compared to a smaller model\n",
    "    - larger models achieve the loss with fewer tokens (more data efficient)\n",
    "    - **but** with a larger compute requirement (higher **training budget**)\n",
    "    - a smaller model can achieve the same Loss\n",
    "        - using more training tokens\n",
    "        - **and** is more cost effective at inference time\n",
    "        \n",
    "Thus, the trend is toward smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
